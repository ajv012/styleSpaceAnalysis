{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Anurag Vaidya  \n",
    "Date: 2/18/2022  \n",
    "Lab: Polina Lab @ CSAIL  \n",
    "Purpose: Create an encoer-generator (stylegan) connected to a clf. Trying to recreate the method from [StyleEx](https://arxiv.org/pdf/2104.13369.pdf)\n",
    "\n",
    "## Notebook Structure\n",
    "- Imports\n",
    "- Args\n",
    "- Dataset\n",
    "- Model\n",
    "- Training/ Val loop\n",
    "- main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as Fun\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from argparse import Namespace\n",
    "import time, copy\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "# wandb.init(project=\"cat-dog-styleSpace\", entity=\"stylespace\")\n",
    "\n",
    "\n",
    "sys.path.append(\"./\")\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9455/86935368.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "os.path.dirname(__file__) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                 train_dir = \"../data/afhq/train\",\n",
    "                 val_dir = \"../data/afhq/val\",\n",
    "                 save_path = \"./checkpoints\",\n",
    "                 seed = 7,\n",
    "                 labels = [\"cat\", \"dog\"],\n",
    "                 batch_size = 64,\n",
    "                 epochs = 50,\n",
    "                 num_workers = 0,\n",
    "                 class_names = {0:\"cat\", 1:\"dog\"} ,\n",
    "                 lr = 0.0001,\n",
    "                 momentum = 0.9,\n",
    "                 criterion = nn.CrossEntropyLoss(),\n",
    "                 optimizer = \"SGD\",\n",
    "                 scheduler = \"STEP\",\n",
    "                 scheduler_step_size = 7,\n",
    "                 scheduler_gamma = 0.1,\n",
    "                 exp_name = \"stylespace1\",\n",
    "                 wandb_config = {\"learning_rate\": 0.0001, \"epochs\": 2, \"batch_size\": 64},\n",
    "                 use_wandb = True,\n",
    "                 output_size = 512,\n",
    "                 encoder_type = \"gradual\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### modex class: contains the model (sub-models are encoder, decoder, discriminator), optim, losses, train, val and util methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.wandb_utils import WBLogger\n",
    "class modex:\n",
    "\tdef __init__(self, args):\n",
    "\t\tself.args = args\n",
    "\n",
    "\t\tself.global_step = 0\n",
    "\n",
    "\t\t# TODO: Allow multiple GPU? currently using CUDA_VISIBLE_DEVICES (use distributed module)\n",
    "\t\tself.device = self.args.device  \n",
    "\n",
    "\t\tif self.args.use_wandb:\n",
    "\t\t\t\n",
    "\t\t\tself.wb_logger = WBLogger(self.args)\n",
    "\t\t\n",
    "\t\t# TODO: pick up here after implementing psp\n",
    "\t\t# Initialize network\n",
    "\t\tself.net = pSp(self.opts).to(self.device)\n",
    "\n",
    "\t\t# Estimate latent_avg via dense sampling if latent_avg is not available\n",
    "\t\tif self.net.latent_avg is None:\n",
    "\t\t\tself.net.latent_avg = self.net.decoder.mean_latent(int(1e5))[0].detach()\n",
    "\n",
    "\t\t# Initialize loss\n",
    "\t\tif self.opts.id_lambda > 0 and self.opts.moco_lambda > 0:\n",
    "\t\t\traise ValueError('Both ID and MoCo loss have lambdas > 0! Please select only one to have non-zero lambda!')\n",
    "\n",
    "\t\tself.mse_loss = nn.MSELoss().to(self.device).eval()\n",
    "\t\tif self.opts.lpips_lambda > 0:\n",
    "\t\t\tself.lpips_loss = LPIPS(net_type='alex').to(self.device).eval()\n",
    "\t\tif self.opts.id_lambda > 0:\n",
    "\t\t\tself.id_loss = id_loss.IDLoss().to(self.device).eval()\n",
    "\t\tif self.opts.w_norm_lambda > 0:\n",
    "\t\t\tself.w_norm_loss = w_norm.WNormLoss(start_from_latent_avg=self.opts.start_from_latent_avg)\n",
    "\t\tif self.opts.moco_lambda > 0:\n",
    "\t\t\tself.moco_loss = moco_loss.MocoLoss().to(self.device).eval()\n",
    "\n",
    "\t\t# Initialize optimizer\n",
    "\t\tself.optimizer = self.configure_optimizers()\n",
    "\n",
    "\t\t# Initialize dataset\n",
    "\t\tself.train_dataset, self.test_dataset = self.configure_datasets()\n",
    "\t\tself.train_dataloader = DataLoader(self.train_dataset,\n",
    "\t\t\t\t\t\t\t\t\t\t\tbatch_size=self.opts.batch_size,\n",
    "\t\t\t\t\t\t\t\t\t\t\tshuffle=True,\n",
    "\t\t\t\t\t\t\t\t\t\t\tnum_workers=int(self.opts.workers),\n",
    "\t\t\t\t\t\t\t\t\t\t\tdrop_last=True)\n",
    "\t\tself.test_dataloader = DataLoader(self.test_dataset,\n",
    "\t\t\t\t\t\t\t\t\t\t\tbatch_size=self.opts.test_batch_size,\n",
    "\t\t\t\t\t\t\t\t\t\t\tshuffle=False,\n",
    "\t\t\t\t\t\t\t\t\t\t\tnum_workers=int(self.opts.test_workers),\n",
    "\t\t\t\t\t\t\t\t\t\t\tdrop_last=True)\n",
    "\n",
    "\t\t# Initialize logger\n",
    "\t\tlog_dir = os.path.join(opts.exp_dir, 'logs')\n",
    "\t\tos.makedirs(log_dir, exist_ok=True)\n",
    "\t\tself.logger = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "\t\t# Initialize checkpoint dir\n",
    "\t\tself.checkpoint_dir = os.path.join(opts.exp_dir, 'checkpoints')\n",
    "\t\tos.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "\t\tself.best_val_loss = None\n",
    "\t\tif self.opts.save_interval is None:\n",
    "\t\t\tself.opts.save_interval = self.opts.max_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### net class: this class will encapsulate all of the sub-models in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_keys(d, name):\n",
    "\tif 'state_dict' in d:\n",
    "\t\td = d['state_dict']\n",
    "\td_filt = {k[len(name) + 1:]: v for k, v in d.items() if k[:len(name)] == name}\n",
    "\treturn d_filt\n",
    "\n",
    "\n",
    "class net(nn.Module):\n",
    "\n",
    "\tdef __init__(self, args):\n",
    "\t\tsuper(net, self).__init__()\n",
    "\t\tself.set_opts(args)\n",
    "\n",
    "\t\t# compute number of style inputs based on the output resolution\n",
    "\t\tself.opts.n_styles = int(math.log(self.args.output_size, 2)) * 2 - 2\n",
    "\n",
    "\t\t# Define architecture\n",
    "\t\tself.encoder = self.set_encoder()\n",
    "\t\t\n",
    "\t\tself.decoder = Generator(self.opts.output_size, 512, 8)\n",
    "\t\tself.face_pool = torch.nn.AdaptiveAvgPool2d((256, 256))\n",
    "\t\t# Load weights if needed\n",
    "\t\tself.load_weights()\n",
    "\n",
    "\tdef set_encoder(self):\n",
    "\t\tif self.opts.encoder == 'gradual':\n",
    "\t\t\tencoder = psp_encoders.GradualStyleEncoder(50, 'ir_se', self.opts)\n",
    "\t\t\n",
    "\t\t\t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\treturn encoder\n",
    "\n",
    "\tdef load_weights(self):\n",
    "\t\tif self.opts.checkpoint_path is not None:\n",
    "\t\t\tprint('Loading pSp from checkpoint: {}'.format(self.opts.checkpoint_path))\n",
    "\t\t\tckpt = torch.load(self.opts.checkpoint_path, map_location='cpu')\n",
    "\t\t\tself.encoder.load_state_dict(get_keys(ckpt, 'encoder'), strict=True)\n",
    "\t\t\tself.decoder.load_state_dict(get_keys(ckpt, 'decoder'), strict=True)\n",
    "\t\t\tself.__load_latent_avg(ckpt)\n",
    "\t\telse:\n",
    "\t\t\tprint('Loading encoders weights from irse50!')\n",
    "\t\t\tencoder_ckpt = torch.load(model_paths['ir_se50'])\n",
    "\t\t\t# if input to encoder is not an RGB image, do not load the input layer weights\n",
    "\t\t\tif self.opts.label_nc != 0:\n",
    "\t\t\t\tencoder_ckpt = {k: v for k, v in encoder_ckpt.items() if \"input_layer\" not in k}\n",
    "\t\t\tself.encoder.load_state_dict(encoder_ckpt, strict=False)\n",
    "\t\t\tprint('Loading decoder weights from pretrained!')\n",
    "\t\t\tckpt = torch.load(self.opts.stylegan_weights)\n",
    "\t\t\tself.decoder.load_state_dict(ckpt['g_ema'], strict=False)\n",
    "\t\t\tif self.opts.learn_in_w:\n",
    "\t\t\t\tself.__load_latent_avg(ckpt, repeat=1)\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.__load_latent_avg(ckpt, repeat=self.opts.n_styles)\n",
    "\n",
    "\tdef forward(self, x, resize=True, latent_mask=None, input_code=False, randomize_noise=True,\n",
    "\t            inject_latent=None, return_latents=False, alpha=None):\n",
    "\t\tif input_code:\n",
    "\t\t\tcodes = x\n",
    "\t\telse:\n",
    "\t\t\tcodes = self.encoder(x)\n",
    "\t\t\t# normalize with respect to the center of an average face\n",
    "\t\t\tif self.opts.start_from_latent_avg:\n",
    "\t\t\t\tif self.opts.learn_in_w:\n",
    "\t\t\t\t\tcodes = codes + self.latent_avg.repeat(codes.shape[0], 1)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcodes = codes + self.latent_avg.repeat(codes.shape[0], 1, 1)\n",
    "\n",
    "\n",
    "\t\tif latent_mask is not None:\n",
    "\t\t\tfor i in latent_mask:\n",
    "\t\t\t\tif inject_latent is not None:\n",
    "\t\t\t\t\tif alpha is not None:\n",
    "\t\t\t\t\t\tcodes[:, i] = alpha * inject_latent[:, i] + (1 - alpha) * codes[:, i]\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tcodes[:, i] = inject_latent[:, i]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcodes[:, i] = 0\n",
    "\n",
    "\t\tinput_is_latent = not input_code\n",
    "\t\timages, result_latent = self.decoder([codes],\n",
    "\t\t                                     input_is_latent=input_is_latent,\n",
    "\t\t                                     randomize_noise=randomize_noise,\n",
    "\t\t                                     return_latents=return_latents)\n",
    "\n",
    "\t\tif resize:\n",
    "\t\t\timages = self.face_pool(images)\n",
    "\n",
    "\t\tif return_latents:\n",
    "\t\t\treturn images, result_latent\n",
    "\t\telse:\n",
    "\t\t\treturn images\n",
    "\n",
    "\tdef set_opts(self, args):\n",
    "\t\tself.args = args\n",
    "\n",
    "\tdef __load_latent_avg(self, ckpt, repeat=None):\n",
    "\t\tif 'latent_avg' in ckpt:\n",
    "\t\t\tself.latent_avg = ckpt['latent_avg'].to(self.opts.device)\n",
    "\t\t\tif repeat is not None:\n",
    "\t\t\t\tself.latent_avg = self.latent_avg.repeat(repeat, 1)\n",
    "\t\telse:\n",
    "\t\t\tself.latent_avg = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d, BatchNorm2d, PReLU, Sequential, Module\n",
    "\n",
    "class GradualStyleBlock(Module):\n",
    "    def __init__(self, in_c, out_c, spatial):\n",
    "        super(GradualStyleBlock, self).__init__()\n",
    "        self.out_c = out_c\n",
    "        self.spatial = spatial\n",
    "        num_pools = int(np.log2(spatial))\n",
    "        modules = []\n",
    "        modules += [Conv2d(in_c, out_c, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.LeakyReLU()]\n",
    "        for i in range(num_pools - 1):\n",
    "            modules += [\n",
    "                Conv2d(out_c, out_c, kernel_size=3, stride=2, padding=1),\n",
    "                nn.LeakyReLU()\n",
    "            ]\n",
    "        self.convs = nn.Sequential(*modules)\n",
    "        self.linear = EqualLinear(out_c, out_c, lr_mul=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self.out_c)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GradualStyleEncoder(Module):\n",
    "    def __init__(self, num_layers, mode='ir', opts=None):\n",
    "        super(GradualStyleEncoder, self).__init__()\n",
    "        assert num_layers in [50, 100, 152], 'num_layers should be 50,100, or 152'\n",
    "        assert mode in ['ir', 'ir_se'], 'mode should be ir or ir_se'\n",
    "        blocks = get_blocks(num_layers)\n",
    "        if mode == 'ir':\n",
    "            unit_module = bottleneck_IR\n",
    "        elif mode == 'ir_se':\n",
    "            unit_module = bottleneck_IR_SE\n",
    "        self.input_layer = Sequential(Conv2d(opts.input_nc, 64, (3, 3), 1, 1, bias=False),\n",
    "                                      BatchNorm2d(64),\n",
    "                                      PReLU(64))\n",
    "        modules = []\n",
    "        for block in blocks:\n",
    "            for bottleneck in block:\n",
    "                modules.append(unit_module(bottleneck.in_channel,\n",
    "                                           bottleneck.depth,\n",
    "                                           bottleneck.stride))\n",
    "        self.body = Sequential(*modules)\n",
    "\n",
    "        self.styles = nn.ModuleList()\n",
    "        self.style_count = opts.n_styles\n",
    "        self.coarse_ind = 3\n",
    "        self.middle_ind = 7\n",
    "        for i in range(self.style_count):\n",
    "            if i < self.coarse_ind:\n",
    "                style = GradualStyleBlock(512, 512, 16)\n",
    "            elif i < self.middle_ind:\n",
    "                style = GradualStyleBlock(512, 512, 32)\n",
    "            else:\n",
    "                style = GradualStyleBlock(512, 512, 64)\n",
    "            self.styles.append(style)\n",
    "        self.latlayer1 = nn.Conv2d(256, 512, kernel_size=1, stride=1, padding=0)\n",
    "        self.latlayer2 = nn.Conv2d(128, 512, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def _upsample_add(self, x, y):\n",
    "        '''Upsample and add two feature maps.\n",
    "        Args:\n",
    "          x: (Variable) top feature map to be upsampled.\n",
    "          y: (Variable) lateral feature map.\n",
    "        Returns:\n",
    "          (Variable) added feature map.\n",
    "        Note in PyTorch, when input size is odd, the upsampled feature map\n",
    "        with `F.upsample(..., scale_factor=2, mode='nearest')`\n",
    "        maybe not equal to the lateral feature map size.\n",
    "        e.g.\n",
    "        original input size: [N,_,15,15] ->\n",
    "        conv2d feature map size: [N,_,8,8] ->\n",
    "        upsampled feature map size: [N,_,16,16]\n",
    "        So we choose bilinear upsample which supports arbitrary output sizes.\n",
    "        '''\n",
    "        _, _, H, W = y.size()\n",
    "        return F.interpolate(x, size=(H, W), mode='bilinear', align_corners=True) + y\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        latents = []\n",
    "        modulelist = list(self.body._modules.values())\n",
    "        for i, l in enumerate(modulelist):\n",
    "            x = l(x)\n",
    "            if i == 6:\n",
    "                c1 = x\n",
    "            elif i == 20:\n",
    "                c2 = x\n",
    "            elif i == 23:\n",
    "                c3 = x\n",
    "\n",
    "        for j in range(self.coarse_ind):\n",
    "            latents.append(self.styles[j](c3))\n",
    "\n",
    "        p2 = self._upsample_add(c3, self.latlayer1(c2))\n",
    "        for j in range(self.coarse_ind, self.middle_ind):\n",
    "            latents.append(self.styles[j](p2))\n",
    "\n",
    "        p1 = self._upsample_add(p2, self.latlayer2(c1))\n",
    "        for j in range(self.middle_ind, self.style_count):\n",
    "            latents.append(self.styles[j](p1))\n",
    "\n",
    "        out = torch.stack(latents, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# styleGan2\n",
    "\n",
    "class EqualLinear(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_dim, out_dim, bias=True, bias_init=0, lr_mul=1, activation=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(out_dim, in_dim).div_(lr_mul))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_dim).fill_(bias_init))\n",
    "\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        self.scale = (1 / math.sqrt(in_dim)) * lr_mul\n",
    "        self.lr_mul = lr_mul\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.activation:\n",
    "            out = F.linear(input, self.weight * self.scale)\n",
    "            out = fused_leaky_relu(out, self.bias * self.lr_mul)\n",
    "\n",
    "        else:\n",
    "            out = F.linear(\n",
    "                input, self.weight * self.scale, bias=self.bias * self.lr_mul\n",
    "            )\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "class WBLogger:\n",
    "\n",
    "    def __init__(self, args):\n",
    "        wandb_run_name = args.exp_name\n",
    "        wandb.init(project=\"style_space_run\", config=vars(args.wandb_config), name=wandb_run_name, entity=\"stylespace\")\n",
    "\n",
    "    @staticmethod\n",
    "    def log_best_model():\n",
    "        wandb.run.summary[\"best-model-save-time\"] = datetime.datetime.now()\n",
    "\n",
    "    @staticmethod\n",
    "    # method should log all relevant metrics for a run\n",
    "    # TODO: update after code is written\n",
    "    def log(prefix, metrics_dict, global_step):\n",
    "        log_dict = {f'{prefix}_{key}': value for key, value in metrics_dict.items()} # figure out what metrics dict is\n",
    "        log_dict[\"global_step\"] = global_step\n",
    "        wandb.log(log_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ops for stylegan2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "from torch.utils.cpp_extension import load\n",
    "\n",
    "module_path = os.path.dirname(__file__) # dirname will get the parent directory name of the current file (__file__)\n",
    "# TODO: need these fused_bias files\n",
    "fused = load(\n",
    "    'fused',\n",
    "    sources=[\n",
    "        os.path.join(module_path, 'fused_bias_act.cpp'),\n",
    "        os.path.join(module_path, 'fused_bias_act_kernel.cu'),\n",
    "    ],\n",
    ")\n",
    "\n",
    "class FusedLeakyReLUFunctionBackward(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, grad_output, out, negative_slope, scale):\n",
    "        ctx.save_for_backward(out)\n",
    "        ctx.negative_slope = negative_slope\n",
    "        ctx.scale = scale\n",
    "\n",
    "        empty = grad_output.new_empty(0)\n",
    "\n",
    "        grad_input = fused.fused_bias_act(\n",
    "            grad_output, empty, out, 3, 1, negative_slope, scale\n",
    "        )\n",
    "\n",
    "        dim = [0]\n",
    "\n",
    "        if grad_input.ndim > 2:\n",
    "            dim += list(range(2, grad_input.ndim))\n",
    "\n",
    "        grad_bias = grad_input.sum(dim).detach()\n",
    "\n",
    "        return grad_input, grad_bias\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, gradgrad_input, gradgrad_bias):\n",
    "        out, = ctx.saved_tensors\n",
    "        gradgrad_out = fused.fused_bias_act(\n",
    "            gradgrad_input, gradgrad_bias, out, 3, 1, ctx.negative_slope, ctx.scale\n",
    "        )\n",
    "\n",
    "        return gradgrad_out, None, None, None\n",
    "\n",
    "\n",
    "class FusedLeakyReLUFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, bias, negative_slope, scale):\n",
    "        empty = input.new_empty(0)\n",
    "        out = fused.fused_bias_act(input, bias, empty, 3, 0, negative_slope, scale)\n",
    "        ctx.save_for_backward(out)\n",
    "        ctx.negative_slope = negative_slope\n",
    "        ctx.scale = scale\n",
    "\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        out, = ctx.saved_tensors\n",
    "\n",
    "        grad_input, grad_bias = FusedLeakyReLUFunctionBackward.apply(\n",
    "            grad_output, out, ctx.negative_slope, ctx.scale\n",
    "        )\n",
    "\n",
    "        return grad_input, grad_bias, None, None\n",
    "\n",
    "class FusedLeakyReLU(nn.Module):\n",
    "    def __init__(self, channel, negative_slope=0.2, scale=2 ** 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(channel))\n",
    "        self.negative_slope = negative_slope\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, input):\n",
    "        return fused_leaky_relu(input, self.bias, self.negative_slope, self.scale)\n",
    "\n",
    "def fused_leaky_relu(input, bias, negative_slope=0.2, scale=2 ** 0.5):\n",
    "    return FusedLeakyReLUFunction.apply(input, bias, negative_slope, scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12c6f07df854dc6c88addbe025d1c4b6056d594a7c27e7c64a55d4c278f1201f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('styleSpace')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
