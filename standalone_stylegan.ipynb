{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Anurag Vaidya  \n",
    "Date: 2/18/2022  \n",
    "Lab: Polina Lab @ CSAIL  \n",
    "Purpose: Create an encoer-generator (stylegan) connected to a clf. Trying to recreate the method from [StyleEx](https://arxiv.org/pdf/2104.13369.pdf)\n",
    "\n",
    "## Notebook Structure\n",
    "- Imports\n",
    "- Args\n",
    "- Dataset\n",
    "- Model\n",
    "- Training/ Val loop\n",
    "- main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as Fun\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch import autograd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from argparse import Namespace\n",
    "import time, copy\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "# wandb.init(project=\"cat-dog-styleSpace\", entity=\"stylespace\")\n",
    "\n",
    "\n",
    "sys.path.append(\"./\")\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                 train_dir = \"../data/afhq/train\",\n",
    "                 val_dir = \"../data/afhq/val\",\n",
    "                 save_path = \"./checkpoints\",\n",
    "                 log_dir = \"./\",\n",
    "                 seed = 7,\n",
    "                 labels = [\"cat\", \"dog\"],\n",
    "                 batch_size = 64,\n",
    "                 test_batch_size = 64,\n",
    "                 epochs = 50,\n",
    "                 num_workers = 0,\n",
    "                 class_names = {0:\"cat\", 1:\"dog\"} ,\n",
    "                 lr = 0.0001,\n",
    "                 lr_d = 0.0004,\n",
    "                 momentum = 0.9,\n",
    "                 criterion = nn.CrossEntropyLoss(),\n",
    "                 optim_name = \"ranger\",\n",
    "                 scheduler = \"STEP\",\n",
    "                 scheduler_step_size = 7,\n",
    "                 scheduler_gamma = 0.1,\n",
    "                 exp_name = \"stylespace1\",\n",
    "                 wandb_config = {\"learning_rate\": 0.0001, \"epochs\": 2, \"batch_size\": 64},\n",
    "                 use_wandb = True,\n",
    "                 output_size = 512,\n",
    "                 encoder_type = \"gradual\",\n",
    "                 n_styles = 0,\n",
    "                 lambdas = {\"adv\":1, \"reg\":1, \"rec_x\":1, \"rec_w\":1, \"lpips\":1, \"clf\":1},\n",
    "                 train_decoder = True, # whether to train decoder,\n",
    "                 dataset_type = \"afhq\",\n",
    "                 max_steps = 50000, # max number of training steps,\n",
    "                 save_interval = 5000, # checkpoint saving interval,\n",
    "                 start_from_latent_avg = False, #Whether to add average latent vector to generate codes from encoder\n",
    "                 learn_in_w = True, # Whether to learn in w space instead of w+,\n",
    "                 img_size = 512, #image sizes for the model\n",
    "                 channel_multiplier = 2, # channel multiplier factor for the model. config-f = 2, else = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### modex class: contains the model (sub-models are encoder, decoder, discriminator), optim, losses, train, val and util methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class modex:\n",
    "\tdef __init__(self, args):\n",
    "\t\tself.args = args\n",
    "\n",
    "\t\tself.global_step = 0\n",
    "\n",
    "\t\t# TODO: Allow multiple GPU? currently using CUDA_VISIBLE_DEVICES (use distributed module)\n",
    "\t\tself.device = self.args.device  \n",
    "\n",
    "\t\tif self.args.use_wandb:\n",
    "\t\t\tself.wb_logger = WBLogger(self.args)\n",
    "\t\t\n",
    "\t\t# Initialize network\n",
    "\t\tself.net = net(self.args).to(self.device)\n",
    "\n",
    "\t\t# Estimate latent_avg via dense sampling if latent_avg is not available\n",
    "\t\tif self.net.latent_avg is None:\n",
    "\t\t\tself.net.latent_avg = self.net.decoder.mean_latent(int(1e5))[0].detach()\n",
    "\n",
    "\t\t# Initialize loss\n",
    "\t\t# adv loss\n",
    "\t\tif self.args.lambdas[\"adv\"] > 0:\n",
    "\t\t\tself.adv_loss = adv_loss().to(self.device) \n",
    "\t\t# path regularization\n",
    "\t\tif self.args.lambdas[\"reg\"] > 0:\n",
    "\t\t\tself.reg_loss = path_reg_loss()\n",
    "\t\t# rec_x\n",
    "\t\tif self.args.lambdas[\"rec_x\"] > 0:\n",
    "\t\t\tself.rec_x_loss = nn.L1Loss().to(self.device)\n",
    "\t\t# lpips\n",
    "\t\tif self.args.lambdas[\"lpips\"] > 0:\n",
    "\t\t\tself.lpips_loss = LPIPS(net_type='alex').to(self.device)\n",
    "\t\t# rec_w\n",
    "\t\tif self.args.lambdas[\"rec_w\"] > 0:\n",
    "\t\t\tself.rec_w_loss = nn.L1Loss().to(self.device)\n",
    "\t\t# clf\n",
    "\t\tif self.args.lambdas[\"clf\"] > 0:\n",
    "\t\t\tself.clf_loss = clf_loss(self.args)\n",
    "\n",
    "\t\t# Initialize optimizer\n",
    "\t\tself.optimizer_g, self.optimizer_d = self.configure_optimizers()\n",
    "\n",
    "\t\t# Initialize dataset\n",
    "\t\tself.train_dataset, self.test_dataset = self.configure_datasets()\n",
    "\t\tself.train_dataloader = DataLoader(self.train_dataset,\n",
    "\t\t\t\t\t\t\t\t\t\t\tbatch_size=self.args.batch_size,\n",
    "\t\t\t\t\t\t\t\t\t\t\tshuffle=True,\n",
    "\t\t\t\t\t\t\t\t\t\t\tnum_workers=int(self.opts.workers),\n",
    "\t\t\t\t\t\t\t\t\t\t\tdrop_last=True)\n",
    "\t\tself.test_dataloader = DataLoader(self.test_dataset,\n",
    "\t\t\t\t\t\t\t\t\t\t\tbatch_size=self.args.test_batch_size,\n",
    "\t\t\t\t\t\t\t\t\t\t\tshuffle=False,\n",
    "\t\t\t\t\t\t\t\t\t\t\tnum_workers=int(self.opts.test_workers),\n",
    "\t\t\t\t\t\t\t\t\t\t\tdrop_last=True)\n",
    "\n",
    "\t\t# Initialize logger\n",
    "\t\tlog_dir = os.path.join(self.args.log_dir, 'logs')\n",
    "\t\tos.makedirs(log_dir, exist_ok=True)\n",
    "\t\tself.logger = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "\t\t# Initialize checkpoint dir\n",
    "\t\tself.checkpoint_dir = os.path.join(self.args.log_dir, 'checkpoints', 'cat_dog_styleGAN')\n",
    "\t\tos.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "\t\tself.best_val_loss = None\n",
    "\t\tif self.args.save_interval is None:\n",
    "\t\t\tself.args.save_interval = self.args.max_steps\n",
    "\n",
    "\tdef configure_optimizers(self):\n",
    "\t\t# encoder + decoder optim\n",
    "\t\tparams_g = list(self.net.encoder.parameters())\n",
    "\t\tif self.args.train_decoder:\n",
    "\t\t\tparams_g += list(self.net.decoder.parameters())\n",
    "\t\tif self.args.optim_name == 'adam':\n",
    "\t\t\tg_optimizer = torch.optim.Adam(params_g, lr=self.args.lr)\n",
    "\t\telse:\n",
    "\t\t\tg_optimizer = Ranger(params_g, lr=self.args.lr)\n",
    "\n",
    "\t\t# discriminator optim\n",
    "\t\tparams_d = self.net.discriminator.parameters()\n",
    "\t\td_optimizer = optim.Adam(params_d, lr=self.args.lr_d)\n",
    "\n",
    "\t\treturn g_optimizer, d_optimizer\n",
    "\n",
    "\tdef configure_datasets(self):\n",
    "\t\tprint(f'Loading dataset for {self.args.dataset_type}')\n",
    "\t\tdataset_args = DATASETS[self.args.dataset_type]\n",
    "\t\ttransforms_dict = dataset_args['transforms'](self.args).get_transforms()\n",
    "\t\ttrain_dataset = afhq_dataset(dataset_args[\"train_dir\"],\n",
    "\t\t\t\t\t\t\t\t\t dataset_args[\"seed\"], \n",
    "\t\t\t\t\t\t\t\t\t dataset_args[\"labels\"], \n",
    "\t\t\t\t\t\t\t\t\t transforms_dict[\"transform_train\"])\n",
    "\t\t\t\t\t\t\t\t\t\n",
    "\t\tval_dataset = afhq_dataset(dataset_args[\"val_dir\"],\n",
    "\t\t\t\t\t\t\t\t\t dataset_args[\"seed\"], \n",
    "\t\t\t\t\t\t\t\t\t dataset_args[\"labels\"], \n",
    "\t\t\t\t\t\t\t\t\t transforms_dict[\"transform_val\"])\n",
    "\t\tif self.args.use_wandb:\n",
    "\t\t\tself.wb_logger.log_dataset_wandb(train_dataset, dataset_name=\"Train\")\n",
    "\t\t\tself.wb_logger.log_dataset_wandb(val_dataset, dataset_name=\"Val\")\n",
    "\t\tprint(f\"Number of training samples: {len(train_dataset)}\")\n",
    "\t\tprint(f\"Number of test samples: {len(val_dataset)}\")\n",
    "\t\treturn train_dataset, val_dataset\n",
    "\n",
    "\tdef requires_grad(self, model, flag=True):\n",
    "\t\tfor p in model.parameters():\n",
    "\t\t\tp.requires_grad = flag\n",
    "\n",
    "\tdef train(self):\n",
    "\t\tself.net.train()\n",
    "\t\tmean_path_length = 0 \n",
    "\t\twhile self.global_step < self.args.max_steps:\n",
    "\n",
    "\t\t\tfor batch_idx, batch in enumerate(self.train_dataloader):\n",
    "\n",
    "\t\t\t\tx, y = batch[\"inputs\"], batch[\"labels\"]\n",
    "\t\t\t\tx, y = x.to(self.device).float(), y.to(self.device).float()\n",
    "\n",
    "\t\t\t\tself.optimizer_g.zero_grad()\n",
    "\t\t\t\tself.optimizer_d.zero_grad()\n",
    "\n",
    "\t\t\t\t########## Discriminator training ##########\n",
    "\t\t\t\tself.requires_grad(self.net.decoder, False)\n",
    "\t\t\t\tself.requires_grad(self.net.discriminator, True)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# get fake img, return_latent = False\n",
    "\t\t\t\ty_hat = self.net.decoder(x)\n",
    "\n",
    "\t\t\t\t# get discriminator outputs\n",
    "\t\t\t\tfake_pred = self.net.discriminator(y_hat)\n",
    "\t\t\t\treal_pred = self.net.discriminator(x) \n",
    "\n",
    "\t\t\t\t# get discriminator loss (pick up here)\n",
    "\t\t\t\tdiscriminator_loss, _, _ = self.calc_loss(x, y, y_hat, latent, fake_pred, real_pred, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  w_fake, w_real, mean_path_length, loss_type=[\"adv\"])\t\n",
    "\n",
    "\t\t\t\t# discriminator updates\n",
    "\t\t\t\tself.net.discriminator.zero_grad()\t\n",
    "\t\t\t\tdiscriminator_loss.backward()\n",
    "\t\t\t\tself.optimizer_d.step()\t\t\n",
    "\n",
    "\t\t\t\t########## Generator + encoder training ##########\n",
    "\t\t\t\tself.requires_grad(self.net.decoder, True)\n",
    "\t\t\t\tself.requires_grad(self.net.discriminator, False)\n",
    "\n",
    "\t\t\t\t# passing through encoder and generator (style gan)\n",
    "\t\t\t\ty_hat, latent = self.net.forward(x, return_latents=True)\n",
    "\n",
    "\t\t\t\t# get encodings\n",
    "\t\t\t\tself.net.encoder.eval()\n",
    "\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\tw_fake = self.net.get_encodings(y_hat)\n",
    "\t\t\t\t\tw_real = self.net.get_encodings(x)\n",
    "\t\t\t\tself.net.encoder.train()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# calculate losses\n",
    "\t\t\t\twhich_loss = [\"reg\", \"rec_x\", \"lpips\", \"rec_w\", \"clf\"]\n",
    "\t\t\t\tloss, loss_dict, mean_path_length = self.calc_loss(x, y, y_hat, latent, fake_pred, real_pred,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t w_fake, w_real, mean_path_length, loss_type=which_loss)\n",
    "\n",
    "\t\t\t\t# backward and step\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\tself.net.zero_grad()\n",
    "\t\t\t\tself.optimizer_g.step()\n",
    "\n",
    "\t\t\t\t# Logging related\n",
    "\t\t\t\tif self.global_step % self.opts.image_interval == 0 or (self.global_step < 1000 and self.global_step % 25 == 0):\n",
    "\t\t\t\t\tself.parse_and_log_images(id_logs, x, y, y_hat, title='images/train/faces')\n",
    "\t\t\t\tif self.global_step % self.opts.board_interval == 0:\n",
    "\t\t\t\t\tself.print_metrics(loss_dict, prefix='train')\n",
    "\t\t\t\t\tself.log_metrics(loss_dict, prefix='train')\n",
    "\n",
    "\t\t\t\t# Log images of first batch to wandb\n",
    "\t\t\t\tif self.opts.use_wandb and batch_idx == 0:\n",
    "\t\t\t\t\tself.wb_logger.log_images_to_wandb(x, y, y_hat, id_logs, prefix=\"train\", step=self.global_step, opts=self.opts)\n",
    "\n",
    "\t\t\t\t# Validation related\n",
    "\t\t\t\tval_loss_dict = None\n",
    "\t\t\t\tif self.global_step % self.opts.val_interval == 0 or self.global_step == self.opts.max_steps:\n",
    "\t\t\t\t\tval_loss_dict = self.validate()\n",
    "\t\t\t\t\tif val_loss_dict and (self.best_val_loss is None or val_loss_dict['loss'] < self.best_val_loss):\n",
    "\t\t\t\t\t\tself.best_val_loss = val_loss_dict['loss']\n",
    "\t\t\t\t\t\tself.checkpoint_me(val_loss_dict, is_best=True)\n",
    "\n",
    "\t\t\t\tif self.global_step % self.opts.save_interval == 0 or self.global_step == self.opts.max_steps:\n",
    "\t\t\t\t\tif val_loss_dict is not None:\n",
    "\t\t\t\t\t\tself.checkpoint_me(val_loss_dict, is_best=False)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tself.checkpoint_me(loss_dict, is_best=False)\n",
    "\n",
    "\t\t\t\tif self.global_step == self.opts.max_steps:\n",
    "\t\t\t\t\tprint('OMG, finished training!')\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t\tself.global_step += 1\n",
    "\n",
    "\tdef calc_loss(self, x, y, y_hat, latent, fake_pred, real_pred, w_fake, w_real, mean_path_length, loss_type):\n",
    "\t\tr\"\"\"\n",
    "\t\tloss_type is a list\n",
    "\t\t\"\"\"\n",
    "\t\tloss_dict = {}\n",
    "\t\tloss = 0.0\n",
    "\t\ttypes = [\"adv\", \"reg\", \"rec_x\", \"lpips\", \"rec_w\", \"clf\"]\n",
    "\t\t\n",
    "\n",
    "\t\tfor curr_loss_name in loss_type:\n",
    "\t\t\tassert loss_type in types, \"Invalid loss name\"\n",
    "\t\t\t# adversarial loss\n",
    "\t\t\tif curr_loss_name == \"adv\":\n",
    "\t\t\t\tloss_adv = self.adv_loss(real_pred, fake_pred)\n",
    "\t\t\t\tloss_dict[\"adv_loss\"] = loss_adv\n",
    "\t\t\t\tloss += self.args.lambdas[\"adv\"] * loss_adv\n",
    "\t\t\t# path regularization\n",
    "\t\t\tif curr_loss_name == \"reg\":\n",
    "\t\t\t\tloss_reg, mean_path_length, path_lengths = self.reg_loss(y_hat, latent, mean_path_length)\n",
    "\t\t\t\tloss_dict[\"reg\"] = loss_reg\n",
    "\t\t\t\tloss += self.args.lambdas[\"reg\"] * loss_reg\n",
    "\t\t\t# rec_x\n",
    "\t\t\tif curr_loss_name == \"rec_x\":\n",
    "\t\t\t\tloss_rec_x = self.rec_x_loss(x, y_hat)\n",
    "\t\t\t\tloss_dict[\"rec_x\"] = loss_rec_x\n",
    "\t\t\t\tloss += self.args.lambdas[\"rec_x\"] * loss_rec_x\n",
    "\t\t\t# lpips\n",
    "\t\t\tif curr_loss_name == \"lpips\":\n",
    "\t\t\t\tloss_lpips = self.lpips_loss(x, y_hat)\n",
    "\t\t\t\tloss_dict[\"lpips\"] = loss_lpips\n",
    "\t\t\t\tloss += self.args.lambdas[\"lpips\"] * loss_lpips\n",
    "\t\t\t# rec_w\n",
    "\t\t\tif curr_loss_name == \"rec_w\":\n",
    "\t\t\t\tloss_rec_w = self.rec_w_loss(w_fake, w_real)\n",
    "\t\t\t\tloss_dict[\"rec_w\"] = loss_rec_w\n",
    "\t\t\t\tloss += self.args.lambdas[\"rec_w\"] * loss_rec_w\n",
    "\t\t\t# clf\n",
    "\t\t\tif curr_loss_name == \"clf\":\n",
    "\t\t\t\tloss_clf = self.clf_loss(x, y_hat) \n",
    "\t\t\t\tloss_dict[\"clf\"] = loss_clf\n",
    "\t\t\t\tloss += self.args.lambdas[\"clf\"] * loss_clf\n",
    "\n",
    "\t\tloss_dict['loss'] = float(loss)\n",
    "\t\treturn loss, loss_dict, mean_path_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform config\n",
    "from abc import abstractmethod\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class TransformsConfig(object):\n",
    "\n",
    "\tdef __init__(self, opts):\n",
    "\t\tself.opts = opts\n",
    "\n",
    "\t@abstractmethod\n",
    "\tdef get_transforms(self):\n",
    "\t\tpass\n",
    "\n",
    "\n",
    "class afhq_Transforms(TransformsConfig):\n",
    "\n",
    "\tdef __init__(self, args):\n",
    "\t\tsuper(afhq_Transforms, self).__init__(args)\n",
    "\n",
    "\tdef get_transforms(self):\n",
    "\t\ttransforms_dict = {\n",
    "\t\t\t'transform_train': transforms.Compose([\n",
    "\t\t\t\ttransforms.Resize((512)),\n",
    "\t\t\t\ttransforms.RandomHorizontalFlip(0.5),\n",
    "\t\t\t\ttransforms.ToTensor(),\n",
    "\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n",
    "\t\t\t'transform_val': transforms.Compose([\n",
    "\t\t\t\ttransforms.Resize((512)),\n",
    "\t\t\t\ttransforms.ToTensor(),\n",
    "\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n",
    "\t\t\t'transform_inference': transforms.Compose([\n",
    "\t\t\t\ttransforms.Resize((256, 256)),\n",
    "\t\t\t\ttransforms.ToTensor(),\n",
    "\t\t\t\ttransforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\t\t}\n",
    "\t\treturn transforms_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data config\n",
    "DATASETS = {\n",
    "    \"afhq\": {\n",
    "        'transforms': afhq_Transforms,\n",
    "        'train_dir': \"../data/afhq/train\",\n",
    "        \"val_dir\": \"../data/afhq/val\",\n",
    "        'seed': 69,\n",
    "        'labels': [\"cat\", \"dog\"],  \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "import sys\n",
    "sys.path.append(\"./\")\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "class afhq_dataset(Dataset):\n",
    "    r\"\"\"\n",
    "    Take a root dir and return the transformed img and associated label with it\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, seed, labels, img_transform=None):\n",
    "\n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        # this dir has two sub dirs cat and dog. Need to combine them\n",
    "        self.root_dir = root_dir\n",
    "        self.cat_names = os.listdir(os.path.join(self.root_dir, \"cat\"))\n",
    "        self.dog_names = os.listdir(os.path.join(self.root_dir, \"dog\"))\n",
    "        self.all_names = np.asarray(self.cat_names + self.dog_names)\n",
    "        np.random.shuffle(self.all_names)\n",
    "        self.img_transform = img_transform\n",
    "        self.labels = {}\n",
    "        for i in range(len(labels)):\n",
    "            self.labels[labels[i]] = i\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        curr_path = os.path.join(self.root_dir, self.all_names[idx].strip().split(\"_\")[1], self.all_names[idx])\n",
    "        curr_img = Image.open(curr_path)\n",
    "        curr_label = self.labels[self.all_names[idx].strip().split(\"_\")[1]]\n",
    "        \n",
    "        if self.img_transform:\n",
    "            curr_img_transformed = self.img_transform(curr_img)\n",
    "        \n",
    "        return {\"inputs\" : curr_img_transformed, \"labels\" : curr_label} \n",
    "    \n",
    "    def viz_img(self, imgs):\n",
    "        r\"\"\"\n",
    "        Take a tensor or list of tensors and visualize it\n",
    "        \"\"\"\n",
    "        if not isinstance(imgs, list):\n",
    "            imgs = [imgs]\n",
    "        fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "        for i, img in enumerate(imgs):\n",
    "            img = img.detach()\n",
    "            img = F.to_pil_image(img)\n",
    "            axs[0, i].imshow(np.asarray(img))\n",
    "            axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    \n",
    "    def what_labels_mean(self):\n",
    "        return [label + \": \" + str(self.labels[label]) for label in self.labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranger optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranger deep learning optimizer - RAdam + Lookahead + Gradient Centralization, combined into one optimizer.\n",
    "\n",
    "# https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n",
    "# and/or\n",
    "# https://github.com/lessw2020/Best-Deep-Learning-Optimizers\n",
    "\n",
    "# Ranger has now been used to capture 12 records on the FastAI leaderboard.\n",
    "\n",
    "# This version = 20.4.11\n",
    "\n",
    "# Credits:\n",
    "# Gradient Centralization --> https://arxiv.org/abs/2004.01461v2 (a new optimization technique for DNNs), github:  https://github.com/Yonghongwei/Gradient-Centralization\n",
    "# RAdam -->  https://github.com/LiyuanLucasLiu/RAdam\n",
    "# Lookahead --> rewritten by lessw2020, but big thanks to Github @LonePatient and @RWightman for ideas from their code.\n",
    "# Lookahead paper --> MZhang,G Hinton  https://arxiv.org/abs/1907.08610\n",
    "\n",
    "# summary of changes:\n",
    "# 4/11/20 - add gradient centralization option.  Set new testing benchmark for accuracy with it, toggle with use_gc flag at init.\n",
    "# full code integration with all updates at param level instead of group, moves slow weights into state dict (from generic weights),\n",
    "# supports group learning rates (thanks @SHolderbach), fixes sporadic load from saved model issues.\n",
    "# changes 8/31/19 - fix references to *self*.N_sma_threshold;\n",
    "# changed eps to 1e-5 as better default than 1e-8.\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "class Ranger(Optimizer):\n",
    "\n",
    "\tdef __init__(self, params, lr=1e-3,  # lr\n",
    "\t\t\t\t alpha=0.5, k=6, N_sma_threshhold=5,  # Ranger options\n",
    "\t\t\t\t betas=(.95, 0.999), eps=1e-5, weight_decay=0,  # Adam options\n",
    "\t\t\t\t use_gc=True, gc_conv_only=False\n",
    "\t\t\t\t # Gradient centralization on or off, applied to conv layers only or conv + fc layers\n",
    "\t\t\t\t ):\n",
    "\n",
    "\t\t# parameter checks\n",
    "\t\tif not 0.0 <= alpha <= 1.0:\n",
    "\t\t\traise ValueError(f'Invalid slow update rate: {alpha}')\n",
    "\t\tif not 1 <= k:\n",
    "\t\t\traise ValueError(f'Invalid lookahead steps: {k}')\n",
    "\t\tif not lr > 0:\n",
    "\t\t\traise ValueError(f'Invalid Learning Rate: {lr}')\n",
    "\t\tif not eps > 0:\n",
    "\t\t\traise ValueError(f'Invalid eps: {eps}')\n",
    "\n",
    "\t\t# parameter comments:\n",
    "\t\t# beta1 (momentum) of .95 seems to work better than .90...\n",
    "\t\t# N_sma_threshold of 5 seems better in testing than 4.\n",
    "\t\t# In both cases, worth testing on your dataset (.90 vs .95, 4 vs 5) to make sure which works best for you.\n",
    "\n",
    "\t\t# prep defaults and init torch.optim base\n",
    "\t\tdefaults = dict(lr=lr, alpha=alpha, k=k, step_counter=0, betas=betas, N_sma_threshhold=N_sma_threshhold,\n",
    "\t\t\t\t\t\teps=eps, weight_decay=weight_decay)\n",
    "\t\tsuper().__init__(params, defaults)\n",
    "\n",
    "\t\t# adjustable threshold\n",
    "\t\tself.N_sma_threshhold = N_sma_threshhold\n",
    "\n",
    "\t\t# look ahead params\n",
    "\n",
    "\t\tself.alpha = alpha\n",
    "\t\tself.k = k\n",
    "\n",
    "\t\t# radam buffer for state\n",
    "\t\tself.radam_buffer = [[None, None, None] for ind in range(10)]\n",
    "\n",
    "\t\t# gc on or off\n",
    "\t\tself.use_gc = use_gc\n",
    "\n",
    "\t\t# level of gradient centralization\n",
    "\t\tself.gc_gradient_threshold = 3 if gc_conv_only else 1\n",
    "\n",
    "\tdef __setstate__(self, state):\n",
    "\t\tsuper(Ranger, self).__setstate__(state)\n",
    "\n",
    "\tdef step(self, closure=None):\n",
    "\t\tloss = None\n",
    "\n",
    "\t\t# Evaluate averages and grad, update param tensors\n",
    "\t\tfor group in self.param_groups:\n",
    "\n",
    "\t\t\tfor p in group['params']:\n",
    "\t\t\t\tif p.grad is None:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tgrad = p.grad.data.float()\n",
    "\n",
    "\t\t\t\tif grad.is_sparse:\n",
    "\t\t\t\t\traise RuntimeError('Ranger optimizer does not support sparse gradients')\n",
    "\n",
    "\t\t\t\tp_data_fp32 = p.data.float()\n",
    "\n",
    "\t\t\t\tstate = self.state[p]  # get state dict for this param\n",
    "\n",
    "\t\t\t\tif len(state) == 0:  # if first time to run...init dictionary with our desired entries\n",
    "\t\t\t\t\t# if self.first_run_check==0:\n",
    "\t\t\t\t\t# self.first_run_check=1\n",
    "\t\t\t\t\t# print(\"Initializing slow buffer...should not see this at load from saved model!\")\n",
    "\t\t\t\t\tstate['step'] = 0\n",
    "\t\t\t\t\tstate['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "\t\t\t\t\tstate['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "\n",
    "\t\t\t\t\t# look ahead weight storage now in state dict\n",
    "\t\t\t\t\tstate['slow_buffer'] = torch.empty_like(p.data)\n",
    "\t\t\t\t\tstate['slow_buffer'].copy_(p.data)\n",
    "\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tstate['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "\t\t\t\t\tstate['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "\t\t\t\t# begin computations\n",
    "\t\t\t\texp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "\t\t\t\tbeta1, beta2 = group['betas']\n",
    "\n",
    "\t\t\t\t# GC operation for Conv layers and FC layers\n",
    "\t\t\t\tif grad.dim() > self.gc_gradient_threshold:\n",
    "\t\t\t\t\tgrad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))\n",
    "\n",
    "\t\t\t\tstate['step'] += 1\n",
    "\n",
    "\t\t\t\t# compute variance mov avg\n",
    "\t\t\t\texp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "\t\t\t\t# compute mean moving avg\n",
    "\t\t\t\texp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "\t\t\t\tbuffered = self.radam_buffer[int(state['step'] % 10)]\n",
    "\n",
    "\t\t\t\tif state['step'] == buffered[0]:\n",
    "\t\t\t\t\tN_sma, step_size = buffered[1], buffered[2]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tbuffered[0] = state['step']\n",
    "\t\t\t\t\tbeta2_t = beta2 ** state['step']\n",
    "\t\t\t\t\tN_sma_max = 2 / (1 - beta2) - 1\n",
    "\t\t\t\t\tN_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "\t\t\t\t\tbuffered[1] = N_sma\n",
    "\t\t\t\t\tif N_sma > self.N_sma_threshhold:\n",
    "\t\t\t\t\t\tstep_size = math.sqrt(\n",
    "\t\t\t\t\t\t\t(1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (\n",
    "\t\t\t\t\t\t\t\t\t\tN_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tstep_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "\t\t\t\t\tbuffered[2] = step_size\n",
    "\n",
    "\t\t\t\tif group['weight_decay'] != 0:\n",
    "\t\t\t\t\tp_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "\t\t\t\t# apply lr\n",
    "\t\t\t\tif N_sma > self.N_sma_threshhold:\n",
    "\t\t\t\t\tdenom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\t\t\t\t\tp_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tp_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
    "\n",
    "\t\t\t\tp.data.copy_(p_data_fp32)\n",
    "\n",
    "\t\t\t\t# integrated look ahead...\n",
    "\t\t\t\t# we do it at the param level instead of group level\n",
    "\t\t\t\tif state['step'] % group['k'] == 0:\n",
    "\t\t\t\t\tslow_p = state['slow_buffer']  # get access to slow param tensor\n",
    "\t\t\t\t\tslow_p.add_(self.alpha, p.data - slow_p)  # (fast weights - slow weights) * alpha\n",
    "\t\t\t\t\tp.data.copy_(slow_p)  # copy interpolated weights to RAdam param tensor\n",
    "\n",
    "\t\treturn loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criteria/ losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lpips\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def normalize_activation(x, eps=1e-10):\n",
    "    norm_factor = torch.sqrt(torch.sum(x ** 2, dim=1, keepdim=True))\n",
    "    return x / (norm_factor + eps)\n",
    "\n",
    "\n",
    "def get_state_dict(net_type: str = 'alex', version: str = '0.1'):\n",
    "    # build url\n",
    "    url = 'https://raw.githubusercontent.com/richzhang/PerceptualSimilarity/' \\\n",
    "        + f'master/lpips/weights/v{version}/{net_type}.pth'\n",
    "\n",
    "    # download\n",
    "    old_state_dict = torch.hub.load_state_dict_from_url(\n",
    "        url, progress=True,\n",
    "        map_location=None if torch.cuda.is_available() else torch.device('cpu')\n",
    "    )\n",
    "\n",
    "    # rename keys\n",
    "    new_state_dict = OrderedDict()\n",
    "    for key, val in old_state_dict.items():\n",
    "        new_key = key\n",
    "        new_key = new_key.replace('lin', '')\n",
    "        new_key = new_key.replace('model.', '')\n",
    "        new_state_dict[new_key] = val\n",
    "\n",
    "    return new_state_dict\n",
    "\n",
    "####################################\n",
    "\n",
    "from typing import Sequence\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "def get_network(net_type: str):\n",
    "    if net_type == 'alex':\n",
    "        return AlexNet()\n",
    "    elif net_type == 'squeeze':\n",
    "        return SqueezeNet()\n",
    "    elif net_type == 'vgg':\n",
    "        return VGG16()\n",
    "    else:\n",
    "        raise NotImplementedError('choose net_type from [alex, squeeze, vgg].')\n",
    "\n",
    "\n",
    "class LinLayers(nn.ModuleList):\n",
    "    def __init__(self, n_channels_list: Sequence[int]):\n",
    "        super(LinLayers, self).__init__([\n",
    "            nn.Sequential(\n",
    "                nn.Identity(),\n",
    "                nn.Conv2d(nc, 1, 1, 1, 0, bias=False)\n",
    "            ) for nc in n_channels_list\n",
    "        ])\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "class BaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseNet, self).__init__()\n",
    "\n",
    "        # register buffer\n",
    "        self.register_buffer(\n",
    "            'mean', torch.Tensor([-.030, -.088, -.188])[None, :, None, None])\n",
    "        self.register_buffer(\n",
    "            'std', torch.Tensor([.458, .448, .450])[None, :, None, None])\n",
    "\n",
    "    def set_requires_grad(self, state: bool):\n",
    "        for param in chain(self.parameters(), self.buffers()):\n",
    "            param.requires_grad = state\n",
    "\n",
    "    def z_score(self, x: torch.Tensor):\n",
    "        return (x - self.mean) / self.std\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.z_score(x)\n",
    "\n",
    "        output = []\n",
    "        for i, (_, layer) in enumerate(self.layers._modules.items(), 1):\n",
    "            x = layer(x)\n",
    "            if i in self.target_layers:\n",
    "                output.append(normalize_activation(x))\n",
    "            if len(output) == len(self.target_layers):\n",
    "                break\n",
    "        return output\n",
    "\n",
    "\n",
    "class SqueezeNet(BaseNet):\n",
    "    def __init__(self):\n",
    "        super(SqueezeNet, self).__init__()\n",
    "\n",
    "        self.layers = models.squeezenet1_1(True).features\n",
    "        self.target_layers = [2, 5, 8, 10, 11, 12, 13]\n",
    "        self.n_channels_list = [64, 128, 256, 384, 384, 512, 512]\n",
    "\n",
    "        self.set_requires_grad(False)\n",
    "\n",
    "\n",
    "class AlexNet(BaseNet):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "\n",
    "        self.layers = models.alexnet(True).features\n",
    "        self.target_layers = [2, 5, 8, 10, 12]\n",
    "        self.n_channels_list = [64, 192, 384, 256, 256]\n",
    "\n",
    "        self.set_requires_grad(False)\n",
    "\n",
    "\n",
    "class VGG16(BaseNet):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "\n",
    "        self.layers = models.vgg16(True).features\n",
    "        self.target_layers = [4, 9, 16, 23, 30]\n",
    "        self.n_channels_list = [64, 128, 256, 512, 512]\n",
    "\n",
    "        self.set_requires_grad(False)\n",
    "\n",
    "\n",
    "##############################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LPIPS(nn.Module):\n",
    "    r\"\"\"Creates a criterion that measures\n",
    "    Learned Perceptual Image Patch Similarity (LPIPS).\n",
    "    Arguments:\n",
    "        net_type (str): the network type to compare the features:\n",
    "                        'alex' | 'squeeze' | 'vgg'. Default: 'alex'.\n",
    "        version (str): the version of LPIPS. Default: 0.1.\n",
    "    \"\"\"\n",
    "    def __init__(self, net_type: str = 'alex', version: str = '0.1'):\n",
    "\n",
    "        assert version in ['0.1'], 'v0.1 is only supported now'\n",
    "\n",
    "        super(LPIPS, self).__init__()\n",
    "\n",
    "        # pretrained network\n",
    "        self.net = get_network(net_type).to(\"cuda\")\n",
    "\n",
    "        # linear layers\n",
    "        self.lin = LinLayers(self.net.n_channels_list).to(\"cuda\")\n",
    "        self.lin.load_state_dict(get_state_dict(net_type, version))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        feat_x, feat_y = self.net(x), self.net(y)\n",
    "\n",
    "        diff = [(fx - fy) ** 2 for fx, fy in zip(feat_x, feat_y)]\n",
    "        res = [l(d).mean((2, 3), True) for d, l in zip(diff, self.lin)]\n",
    "\n",
    "        return torch.sum(torch.cat(res, 0)) / x.shape[0]\n",
    "\n",
    "############################\n",
    "# adv loss\n",
    "\n",
    "class adv_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(adv_loss, self).__init__()\n",
    "\n",
    "    def forward(self, real_pred: torch.Tensor, fake_pred: torch.Tensor):\n",
    "        real_loss = F.softplus(-real_pred)\n",
    "        fake_loss = F.softplus(fake_pred)\n",
    "\n",
    "        return real_loss.mean() + fake_loss.mean()\n",
    "\n",
    "\n",
    "#############################\n",
    "# clf loss\n",
    "\n",
    "class clf_loss(nn.Module):\n",
    "    def __init__(self, args, num_classes = 2, network = \"resnet\", path_to_weights = \"./checkpoint/checkpoint_2.pt\"):\n",
    "        \n",
    "        super(clf_loss, self).__init__()\n",
    "\n",
    "        self.model_ft = models.resnet18(pretrained=False)\n",
    "        self.num_ftrs = self.model_ft.fc.in_features\n",
    "        self.model_ft.fc = nn.Linear(self.num_ftrs, num_classes)\n",
    "\n",
    "        checkpoint = torch.load(path_to_weights)\n",
    "        self.model_ft.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        self.model_ft.eval()\n",
    "\n",
    "        self.loss_func = nn.KLDivLoss().to(args.device)\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor, y_hat: torch.Tensor):\n",
    "        return self.loss_func(self.model_ft(x), self.model_ft(y_hat))\n",
    "\n",
    "#############################\n",
    "# path reg loss\n",
    "\n",
    "class path_reg_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(path_reg_loss, self).__init__()\n",
    "\n",
    "    def forward(self, fake_img, latents, mean_path_length, decay=0.01):\n",
    "        noise = torch.randn_like(fake_img) / math.sqrt(fake_img.shape[2] * fake_img.shape[3])\n",
    "        grad, = autograd.grad(outputs=(fake_img * noise).sum(), inputs=latents, create_graph=True)\n",
    "        path_lengths = torch.sqrt(grad.pow(2).sum(2).mean(1))\n",
    "\n",
    "        path_mean = mean_path_length + decay * (path_lengths.mean() - mean_path_length)\n",
    "\n",
    "        path_penalty = (path_lengths - path_mean).pow(2).mean()\n",
    "\n",
    "        return path_penalty, path_mean.detach(), path_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### net class: this class will encapsulate all of the sub-models in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_keys(d, name):\n",
    "\tif 'state_dict' in d:\n",
    "\t\td = d['state_dict']\n",
    "\td_filt = {k[len(name) + 1:]: v for k, v in d.items() if k[:len(name)] == name}\n",
    "\treturn d_filt\n",
    "\n",
    "\n",
    "class net(nn.Module):\n",
    "\n",
    "\tdef __init__(self, args):\n",
    "\t\tsuper(net, self).__init__()\n",
    "\t\tself.args = args\n",
    "\n",
    "\t\t# compute number of style inputs based on the output resolution\n",
    "\t\tself.args.n_styles = int(math.log(self.args.output_size, 2)) * 2 - 2\n",
    "\n",
    "\t\t# Define architecture\n",
    "\t\tself.encoder = self.set_encoder()\n",
    "\t\t\n",
    "\t\t# define generator \n",
    "\t\tself.decoder = Generator(self.opts.output_size, 512, 8)\n",
    "\t\tself.face_pool = torch.nn.AdaptiveAvgPool2d((256, 256))\n",
    "\n",
    "\t\t# define discrmiinator\n",
    "\t\tself.discriminator = Discriminator(self.args.img_size, self.channel_multiplier)\n",
    "\n",
    "\t\t# define the classifier \n",
    "\t\tself.classifier = Classifier(self.args)\n",
    "\n",
    "\t\tself.latent_avg = None\n",
    "\n",
    "\t\t# Load weights if needed -> we are going to train from scratch\n",
    "\t\t# self.load_weights()\n",
    "\n",
    "\tdef set_encoder(self):\n",
    "\t\tif self.opts.encoder == 'gradual':\n",
    "\t\t\tencoder = GradualStyleEncoder(50, 'ir_se', self.opts)\n",
    "\t\t\n",
    "\t\treturn encoder\n",
    "\n",
    "\t# def load_weights(self):\n",
    "\t# \tif self.opts.checkpoint_path is not None:\n",
    "\t# \t\tprint('Loading pSp from checkpoint: {}'.format(self.opts.checkpoint_path))\n",
    "\t# \t\tckpt = torch.load(self.opts.checkpoint_path, map_location='cpu')\n",
    "\t# \t\tself.encoder.load_state_dict(get_keys(ckpt, 'encoder'), strict=True)\n",
    "\t# \t\tself.decoder.load_state_dict(get_keys(ckpt, 'decoder'), strict=True)\n",
    "\t# \t\tself.__load_latent_avg(ckpt)\n",
    "\t# \telse:\n",
    "\t# \t\tprint('Loading encoders weights from irse50!')\n",
    "\t# \t\tencoder_ckpt = torch.load(model_paths['ir_se50'])\n",
    "\t# \t\t# if input to encoder is not an RGB image, do not load the input layer weights\n",
    "\t# \t\tif self.opts.label_nc != 0:\n",
    "\t# \t\t\tencoder_ckpt = {k: v for k, v in encoder_ckpt.items() if \"input_layer\" not in k}\n",
    "\t# \t\tself.encoder.load_state_dict(encoder_ckpt, strict=False)\n",
    "\t# \t\tprint('Loading decoder weights from pretrained!')\n",
    "\t# \t\tckpt = torch.load(self.opts.stylegan_weights)\n",
    "\t# \t\tself.decoder.load_state_dict(ckpt['g_ema'], strict=False)\n",
    "\t# \t\tif self.opts.learn_in_w:\n",
    "\t# \t\t\tself.__load_latent_avg(ckpt, repeat=1)\n",
    "\t# \t\telse:\n",
    "\t# \t\t\tself.__load_latent_avg(ckpt, repeat=self.opts.n_styles)\n",
    "\n",
    "\tdef forward(self, x, resize=True, latent_mask=None, input_code=False, randomize_noise=True,\n",
    "\t            inject_latent=None, return_latents=False, alpha=None):\n",
    "\t\tif input_code:\n",
    "\t\t\tcodes = x\n",
    "\t\telse:\n",
    "\t\t\tcodes = self.encoder(x)\n",
    "\t\t\t# normalize with respect to the center of an average face\n",
    "\t\t\t# if True then will need pretrained weights\n",
    "\t\t\tif self.args.start_from_latent_avg:\n",
    "\t\t\t\tif self.args.learn_in_w:\n",
    "\t\t\t\t\tcodes = codes + self.latent_avg.repeat(codes.shape[0], 1)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcodes = codes + self.latent_avg.repeat(codes.shape[0], 1, 1)\n",
    "\n",
    "\n",
    "\t\tif latent_mask is not None:\n",
    "\t\t\tfor i in latent_mask:\n",
    "\t\t\t\tif inject_latent is not None:\n",
    "\t\t\t\t\tif alpha is not None:\n",
    "\t\t\t\t\t\tcodes[:, i] = alpha * inject_latent[:, i] + (1 - alpha) * codes[:, i]\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tcodes[:, i] = inject_latent[:, i]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcodes[:, i] = 0\n",
    "\n",
    "\t\tinput_is_latent = not input_code\n",
    "\n",
    "\t\t### get clf output and concatenate it to the encoder output\n",
    "\t\tclf_out = self.classifier(x)\n",
    "\t\tcodes = torch.cat([codes, x]) # mostly wrong, but troubleshoot as you run the code. Need to know size of codes\n",
    "\t\t\n",
    "\t\timages, result_latent = self.decoder([codes],\n",
    "\t\t                                     input_is_latent=input_is_latent,\n",
    "\t\t                                     randomize_noise=randomize_noise,\n",
    "\t\t                                     return_latents=return_latents)\n",
    "\n",
    "\t\tif resize:\n",
    "\t\t\timages = self.face_pool(images)\n",
    "\n",
    "\t\tif return_latents:\n",
    "\t\t\treturn images, result_latent\n",
    "\t\telse:\n",
    "\t\t\treturn images\n",
    "\n",
    "\tdef get_encodings(self, x):\n",
    "\t\tr\"\"\"\n",
    "\t\tGet the encoding of x. Before coming here, encoder should be set to eval and no_grad should be used\n",
    "\t\t\"\"\"\n",
    "\t\treturn self.encoder(x)\n",
    "\n",
    "\t# def __load_latent_avg(self, ckpt, repeat=None):\n",
    "\t# \tif 'latent_avg' in ckpt:\n",
    "\t# \t\tself.latent_avg = ckpt['latent_avg'].to(self.opts.device)\n",
    "\t# \t\tif repeat is not None:\n",
    "\t# \t\t\tself.latent_avg = self.latent_avg.repeat(repeat, 1)\n",
    "\t# \telse:\n",
    "\t# \t\tself.latent_avg = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier class\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, args, num_classes = 2, network = \"resnet\", path_to_weights = \"./checkpoint/checkpoint_2.pt\"):\n",
    "\n",
    "        self.model_ft = models.resnet18(pretrained=False)\n",
    "        self.num_ftrs = self.model_ft.fc.in_features\n",
    "        self.model_ft.fc = nn.Linear(self.num_ftrs, num_classes)\n",
    "\n",
    "        checkpoint = torch.load(path_to_weights)\n",
    "        self.model_ft.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        self.model_ft.eval()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.model_ft(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d, BatchNorm2d, PReLU, Sequential, Module\n",
    "\n",
    "class GradualStyleBlock(Module):\n",
    "    def __init__(self, in_c, out_c, spatial):\n",
    "        super(GradualStyleBlock, self).__init__()\n",
    "        self.out_c = out_c\n",
    "        self.spatial = spatial\n",
    "        num_pools = int(np.log2(spatial))\n",
    "        modules = []\n",
    "        modules += [Conv2d(in_c, out_c, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.LeakyReLU()]\n",
    "        for i in range(num_pools - 1):\n",
    "            modules += [\n",
    "                Conv2d(out_c, out_c, kernel_size=3, stride=2, padding=1),\n",
    "                nn.LeakyReLU()\n",
    "            ]\n",
    "        self.convs = nn.Sequential(*modules)\n",
    "        self.linear = EqualLinear(out_c, out_c, lr_mul=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self.out_c)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GradualStyleEncoder(Module):\n",
    "    def __init__(self, num_layers, mode='ir', opts=None):\n",
    "        super(GradualStyleEncoder, self).__init__()\n",
    "        assert num_layers in [50, 100, 152], 'num_layers should be 50,100, or 152'\n",
    "        assert mode in ['ir', 'ir_se'], 'mode should be ir or ir_se'\n",
    "        blocks = get_blocks(num_layers)\n",
    "        if mode == 'ir':\n",
    "            unit_module = bottleneck_IR\n",
    "        elif mode == 'ir_se':\n",
    "            unit_module = bottleneck_IR_SE\n",
    "        self.input_layer = Sequential(Conv2d(opts.input_nc, 64, (3, 3), 1, 1, bias=False),\n",
    "                                      BatchNorm2d(64),\n",
    "                                      PReLU(64))\n",
    "        modules = []\n",
    "        for block in blocks:\n",
    "            for bottleneck in block:\n",
    "                modules.append(unit_module(bottleneck.in_channel,\n",
    "                                           bottleneck.depth,\n",
    "                                           bottleneck.stride))\n",
    "        self.body = Sequential(*modules)\n",
    "\n",
    "        self.styles = nn.ModuleList()\n",
    "        self.style_count = opts.n_styles\n",
    "        self.coarse_ind = 3\n",
    "        self.middle_ind = 7\n",
    "        for i in range(self.style_count):\n",
    "            if i < self.coarse_ind:\n",
    "                style = GradualStyleBlock(512, 512, 16)\n",
    "            elif i < self.middle_ind:\n",
    "                style = GradualStyleBlock(512, 512, 32)\n",
    "            else:\n",
    "                style = GradualStyleBlock(512, 512, 64)\n",
    "            self.styles.append(style)\n",
    "        self.latlayer1 = nn.Conv2d(256, 512, kernel_size=1, stride=1, padding=0)\n",
    "        self.latlayer2 = nn.Conv2d(128, 512, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def _upsample_add(self, x, y):\n",
    "        '''Upsample and add two feature maps.\n",
    "        Args:\n",
    "          x: (Variable) top feature map to be upsampled.\n",
    "          y: (Variable) lateral feature map.\n",
    "        Returns:\n",
    "          (Variable) added feature map.\n",
    "        Note in PyTorch, when input size is odd, the upsampled feature map\n",
    "        with `F.upsample(..., scale_factor=2, mode='nearest')`\n",
    "        maybe not equal to the lateral feature map size.\n",
    "        e.g.\n",
    "        original input size: [N,_,15,15] ->\n",
    "        conv2d feature map size: [N,_,8,8] ->\n",
    "        upsampled feature map size: [N,_,16,16]\n",
    "        So we choose bilinear upsample which supports arbitrary output sizes.\n",
    "        '''\n",
    "        _, _, H, W = y.size()\n",
    "        return F.interpolate(x, size=(H, W), mode='bilinear', align_corners=True) + y\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        latents = []\n",
    "        modulelist = list(self.body._modules.values())\n",
    "        for i, l in enumerate(modulelist):\n",
    "            x = l(x)\n",
    "            if i == 6:\n",
    "                c1 = x\n",
    "            elif i == 20:\n",
    "                c2 = x\n",
    "            elif i == 23:\n",
    "                c3 = x\n",
    "\n",
    "        for j in range(self.coarse_ind):\n",
    "            latents.append(self.styles[j](c3))\n",
    "\n",
    "        p2 = self._upsample_add(c3, self.latlayer1(c2))\n",
    "        for j in range(self.coarse_ind, self.middle_ind):\n",
    "            latents.append(self.styles[j](p2))\n",
    "\n",
    "        p1 = self._upsample_add(p2, self.latlayer2(c1))\n",
    "        for j in range(self.middle_ind, self.style_count):\n",
    "            latents.append(self.styles[j](p1))\n",
    "\n",
    "        out = torch.stack(latents, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# styleGan2\n",
    "\n",
    "class EqualLinear(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_dim, out_dim, bias=True, bias_init=0, lr_mul=1, activation=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(out_dim, in_dim).div_(lr_mul))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_dim).fill_(bias_init))\n",
    "\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        self.scale = (1 / math.sqrt(in_dim)) * lr_mul\n",
    "        self.lr_mul = lr_mul\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.activation:\n",
    "            out = F.linear(input, self.weight * self.scale)\n",
    "            out = fused_leaky_relu(out, self.bias * self.lr_mul)\n",
    "\n",
    "        else:\n",
    "            out = F.linear(\n",
    "                input, self.weight * self.scale, bias=self.bias * self.lr_mul\n",
    "            )\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from torch import autograd\n",
    "from torch.nn import functional as F\n",
    "\n",
    "enabled = True\n",
    "weight_gradients_disabled = False\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def no_weight_gradients():\n",
    "    global weight_gradients_disabled\n",
    "\n",
    "    old = weight_gradients_disabled\n",
    "    weight_gradients_disabled = True\n",
    "    yield\n",
    "    weight_gradients_disabled = old\n",
    "\n",
    "\n",
    "def conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1):\n",
    "    if could_use_op(input):\n",
    "        return conv2d_gradfix(\n",
    "            transpose=False,\n",
    "            weight_shape=weight.shape,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            output_padding=0,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "        ).apply(input, weight, bias)\n",
    "\n",
    "    return F.conv2d(\n",
    "        input=input,\n",
    "        weight=weight,\n",
    "        bias=bias,\n",
    "        stride=stride,\n",
    "        padding=padding,\n",
    "        dilation=dilation,\n",
    "        groups=groups,\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_transpose2d(\n",
    "    input,\n",
    "    weight,\n",
    "    bias=None,\n",
    "    stride=1,\n",
    "    padding=0,\n",
    "    output_padding=0,\n",
    "    groups=1,\n",
    "    dilation=1,\n",
    "):\n",
    "    if could_use_op(input):\n",
    "        return conv2d_gradfix(\n",
    "            transpose=True,\n",
    "            weight_shape=weight.shape,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            output_padding=output_padding,\n",
    "            groups=groups,\n",
    "            dilation=dilation,\n",
    "        ).apply(input, weight, bias)\n",
    "\n",
    "    return F.conv_transpose2d(\n",
    "        input=input,\n",
    "        weight=weight,\n",
    "        bias=bias,\n",
    "        stride=stride,\n",
    "        padding=padding,\n",
    "        output_padding=output_padding,\n",
    "        dilation=dilation,\n",
    "        groups=groups,\n",
    "    )\n",
    "\n",
    "\n",
    "def could_use_op(input):\n",
    "    if (not enabled) or (not torch.backends.cudnn.enabled):\n",
    "        return False\n",
    "\n",
    "    if input.device.type != \"cuda\":\n",
    "        return False\n",
    "\n",
    "    if any(torch.__version__.startswith(x) for x in [\"1.7.\", \"1.8.\"]):\n",
    "        return True\n",
    "\n",
    "    warnings.warn(\n",
    "        f\"conv2d_gradfix not supported on PyTorch {torch.__version__}. Falling back to torch.nn.functional.conv2d().\"\n",
    "    )\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def ensure_tuple(xs, ndim):\n",
    "    xs = tuple(xs) if isinstance(xs, (tuple, list)) else (xs,) * ndim\n",
    "\n",
    "    return xs\n",
    "\n",
    "\n",
    "conv2d_gradfix_cache = dict()\n",
    "\n",
    "\n",
    "def conv2d_gradfix(\n",
    "    transpose, weight_shape, stride, padding, output_padding, dilation, groups\n",
    "):\n",
    "    ndim = 2\n",
    "    weight_shape = tuple(weight_shape)\n",
    "    stride = ensure_tuple(stride, ndim)\n",
    "    padding = ensure_tuple(padding, ndim)\n",
    "    output_padding = ensure_tuple(output_padding, ndim)\n",
    "    dilation = ensure_tuple(dilation, ndim)\n",
    "\n",
    "    key = (transpose, weight_shape, stride, padding, output_padding, dilation, groups)\n",
    "    if key in conv2d_gradfix_cache:\n",
    "        return conv2d_gradfix_cache[key]\n",
    "\n",
    "    common_kwargs = dict(\n",
    "        stride=stride, padding=padding, dilation=dilation, groups=groups\n",
    "    )\n",
    "\n",
    "    def calc_output_padding(input_shape, output_shape):\n",
    "        if transpose:\n",
    "            return [0, 0]\n",
    "\n",
    "        return [\n",
    "            input_shape[i + 2]\n",
    "            - (output_shape[i + 2] - 1) * stride[i]\n",
    "            - (1 - 2 * padding[i])\n",
    "            - dilation[i] * (weight_shape[i + 2] - 1)\n",
    "            for i in range(ndim)\n",
    "        ]\n",
    "\n",
    "    class Conv2d(autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, weight, bias):\n",
    "            if not transpose:\n",
    "                out = F.conv2d(input=input, weight=weight, bias=bias, **common_kwargs)\n",
    "\n",
    "            else:\n",
    "                out = F.conv_transpose2d(\n",
    "                    input=input,\n",
    "                    weight=weight,\n",
    "                    bias=bias,\n",
    "                    output_padding=output_padding,\n",
    "                    **common_kwargs,\n",
    "                )\n",
    "\n",
    "            ctx.save_for_backward(input, weight)\n",
    "\n",
    "            return out\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            input, weight = ctx.saved_tensors\n",
    "            grad_input, grad_weight, grad_bias = None, None, None\n",
    "\n",
    "            if ctx.needs_input_grad[0]:\n",
    "                p = calc_output_padding(\n",
    "                    input_shape=input.shape, output_shape=grad_output.shape\n",
    "                )\n",
    "                grad_input = conv2d_gradfix(\n",
    "                    transpose=(not transpose),\n",
    "                    weight_shape=weight_shape,\n",
    "                    output_padding=p,\n",
    "                    **common_kwargs,\n",
    "                ).apply(grad_output, weight, None)\n",
    "\n",
    "            if ctx.needs_input_grad[1] and not weight_gradients_disabled:\n",
    "                grad_weight = Conv2dGradWeight.apply(grad_output, input)\n",
    "\n",
    "            if ctx.needs_input_grad[2]:\n",
    "                grad_bias = grad_output.sum((0, 2, 3))\n",
    "\n",
    "            return grad_input, grad_weight, grad_bias\n",
    "\n",
    "    class Conv2dGradWeight(autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, grad_output, input):\n",
    "            op = torch._C._jit_get_operation(\n",
    "                \"aten::cudnn_convolution_backward_weight\"\n",
    "                if not transpose\n",
    "                else \"aten::cudnn_convolution_transpose_backward_weight\"\n",
    "            )\n",
    "            flags = [\n",
    "                torch.backends.cudnn.benchmark,\n",
    "                torch.backends.cudnn.deterministic,\n",
    "                torch.backends.cudnn.allow_tf32,\n",
    "            ]\n",
    "            grad_weight = op(\n",
    "                weight_shape,\n",
    "                grad_output,\n",
    "                input,\n",
    "                padding,\n",
    "                stride,\n",
    "                dilation,\n",
    "                groups,\n",
    "                *flags,\n",
    "            )\n",
    "            ctx.save_for_backward(grad_output, input)\n",
    "\n",
    "            return grad_weight\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_grad_weight):\n",
    "            grad_output, input = ctx.saved_tensors\n",
    "            grad_grad_output, grad_grad_input = None, None\n",
    "\n",
    "            if ctx.needs_input_grad[0]:\n",
    "                grad_grad_output = Conv2d.apply(input, grad_grad_weight, None)\n",
    "\n",
    "            if ctx.needs_input_grad[1]:\n",
    "                p = calc_output_padding(\n",
    "                    input_shape=input.shape, output_shape=grad_output.shape\n",
    "                )\n",
    "                grad_grad_input = conv2d_gradfix(\n",
    "                    transpose=(not transpose),\n",
    "                    weight_shape=weight_shape,\n",
    "                    output_padding=p,\n",
    "                    **common_kwargs,\n",
    "                ).apply(grad_output, grad_grad_weight, None)\n",
    "\n",
    "            return grad_grad_output, grad_grad_input\n",
    "\n",
    "    conv2d_gradfix_cache[key] = Conv2d\n",
    "\n",
    "    return Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator and discriminator layers\n",
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input * torch.rsqrt(torch.mean(input ** 2, dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "class ConstantInput(nn.Module):\n",
    "    def __init__(self, channel, size=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input = nn.Parameter(torch.randn(1, channel, size, size))\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch = input.shape[0]\n",
    "        out = self.input.repeat(batch, 1, 1, 1)\n",
    "\n",
    "        return out\n",
    "        \n",
    "def make_kernel(k):\n",
    "    k = torch.tensor(k, dtype=torch.float32)\n",
    "\n",
    "    if k.ndim == 1:\n",
    "        k = k[None, :] * k[:, None]\n",
    "\n",
    "    k /= k.sum()\n",
    "\n",
    "    return k\n",
    "\n",
    "class EqualConv2d(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channel, out_channel, kernel_size, stride=1, padding=0, bias=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(out_channel, in_channel, kernel_size, kernel_size)\n",
    "        )\n",
    "        self.scale = 1 / math.sqrt(in_channel * kernel_size ** 2)\n",
    "\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_channel))\n",
    "\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = conv2d_gradfix.conv2d(\n",
    "            input,\n",
    "            self.weight * self.scale,\n",
    "            bias=self.bias,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding,\n",
    "        )\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"{self.__class__.__name__}({self.weight.shape[1]}, {self.weight.shape[0]},\"\n",
    "            f\" {self.weight.shape[2]}, stride={self.stride}, padding={self.padding})\"\n",
    "        )\n",
    "\n",
    "class ConvLayer(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel,\n",
    "        out_channel,\n",
    "        kernel_size,\n",
    "        downsample=False,\n",
    "        blur_kernel=[1, 3, 3, 1],\n",
    "        bias=True,\n",
    "        activate=True,\n",
    "    ):\n",
    "        layers = []\n",
    "\n",
    "        if downsample:\n",
    "            factor = 2\n",
    "            p = (len(blur_kernel) - factor) + (kernel_size - 1)\n",
    "            pad0 = (p + 1) // 2\n",
    "            pad1 = p // 2\n",
    "\n",
    "            layers.append(Blur(blur_kernel, pad=(pad0, pad1)))\n",
    "\n",
    "            stride = 2\n",
    "            self.padding = 0\n",
    "\n",
    "        else:\n",
    "            stride = 1\n",
    "            self.padding = kernel_size // 2\n",
    "\n",
    "        layers.append(\n",
    "            EqualConv2d(\n",
    "                in_channel,\n",
    "                out_channel,\n",
    "                kernel_size,\n",
    "                padding=self.padding,\n",
    "                stride=stride,\n",
    "                bias=bias and not activate,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if activate:\n",
    "            layers.append(FusedLeakyReLU(out_channel, bias=bias))\n",
    "\n",
    "        super().__init__(*layers)\n",
    "\n",
    "\n",
    "class Blur(nn.Module):\n",
    "    def __init__(self, kernel, pad, upsample_factor=1):\n",
    "        super().__init__()\n",
    "\n",
    "        kernel = make_kernel(kernel)\n",
    "\n",
    "        if upsample_factor > 1:\n",
    "            kernel = kernel * (upsample_factor ** 2)\n",
    "\n",
    "        self.register_buffer('kernel', kernel)\n",
    "\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = upfirdn2d(input, self.kernel, pad=self.pad)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ModulatedConv2d(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channel,\n",
    "            out_channel,\n",
    "            kernel_size,\n",
    "            style_dim,\n",
    "            demodulate=True,\n",
    "            upsample=False,\n",
    "            downsample=False,\n",
    "            blur_kernel=[1, 3, 3, 1],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = 1e-8\n",
    "        self.kernel_size = kernel_size\n",
    "        self.in_channel = in_channel\n",
    "        self.out_channel = out_channel\n",
    "        self.upsample = upsample\n",
    "        self.downsample = downsample\n",
    "\n",
    "        if upsample:\n",
    "            factor = 2\n",
    "            p = (len(blur_kernel) - factor) - (kernel_size - 1)\n",
    "            pad0 = (p + 1) // 2 + factor - 1\n",
    "            pad1 = p // 2 + 1\n",
    "\n",
    "            self.blur = Blur(blur_kernel, pad=(pad0, pad1), upsample_factor=factor)\n",
    "\n",
    "        if downsample:\n",
    "            factor = 2\n",
    "            p = (len(blur_kernel) - factor) + (kernel_size - 1)\n",
    "            pad0 = (p + 1) // 2\n",
    "            pad1 = p // 2\n",
    "\n",
    "            self.blur = Blur(blur_kernel, pad=(pad0, pad1))\n",
    "\n",
    "        fan_in = in_channel * kernel_size ** 2\n",
    "        self.scale = 1 / math.sqrt(fan_in)\n",
    "        self.padding = kernel_size // 2\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(1, out_channel, in_channel, kernel_size, kernel_size)\n",
    "        )\n",
    "\n",
    "        self.modulation = EqualLinear(style_dim, in_channel, bias_init=1)\n",
    "\n",
    "        self.demodulate = demodulate\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f'{self.__class__.__name__}({self.in_channel}, {self.out_channel}, {self.kernel_size}, '\n",
    "            f'upsample={self.upsample}, downsample={self.downsample})'\n",
    "        )\n",
    "\n",
    "    def forward(self, input, style):\n",
    "        batch, in_channel, height, width = input.shape\n",
    "\n",
    "        style = self.modulation(style).view(batch, 1, in_channel, 1, 1)\n",
    "        weight = self.scale * self.weight * style\n",
    "\n",
    "        if self.demodulate:\n",
    "            demod = torch.rsqrt(weight.pow(2).sum([2, 3, 4]) + 1e-8)\n",
    "            weight = weight * demod.view(batch, self.out_channel, 1, 1, 1)\n",
    "\n",
    "        weight = weight.view(\n",
    "            batch * self.out_channel, in_channel, self.kernel_size, self.kernel_size\n",
    "        )\n",
    "\n",
    "        if self.upsample:\n",
    "            input = input.view(1, batch * in_channel, height, width)\n",
    "            weight = weight.view(\n",
    "                batch, self.out_channel, in_channel, self.kernel_size, self.kernel_size\n",
    "            )\n",
    "            weight = weight.transpose(1, 2).reshape(\n",
    "                batch * in_channel, self.out_channel, self.kernel_size, self.kernel_size\n",
    "            )\n",
    "            out = F.conv_transpose2d(input, weight, padding=0, stride=2, groups=batch)\n",
    "            _, _, height, width = out.shape\n",
    "            out = out.view(batch, self.out_channel, height, width)\n",
    "            out = self.blur(out)\n",
    "\n",
    "        elif self.downsample:\n",
    "            input = self.blur(input)\n",
    "            _, _, height, width = input.shape\n",
    "            input = input.view(1, batch * in_channel, height, width)\n",
    "            out = F.conv2d(input, weight, padding=0, stride=2, groups=batch)\n",
    "            _, _, height, width = out.shape\n",
    "            out = out.view(batch, self.out_channel, height, width)\n",
    "\n",
    "        else:\n",
    "            input = input.view(1, batch * in_channel, height, width)\n",
    "            out = F.conv2d(input, weight, padding=self.padding, groups=batch)\n",
    "            _, _, height, width = out.shape\n",
    "            out = out.view(batch, self.out_channel, height, width)\n",
    "\n",
    "        return out\n",
    "\n",
    "class NoiseInjection(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, image, noise=None):\n",
    "        if noise is None:\n",
    "            batch, _, height, width = image.shape\n",
    "            noise = image.new_empty(batch, 1, height, width).normal_()\n",
    "\n",
    "        return image + self.weight * noise\n",
    "\n",
    "class StyledConv(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channel,\n",
    "            out_channel,\n",
    "            kernel_size,\n",
    "            style_dim,\n",
    "            upsample=False,\n",
    "            blur_kernel=[1, 3, 3, 1],\n",
    "            demodulate=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = ModulatedConv2d(\n",
    "            in_channel,\n",
    "            out_channel,\n",
    "            kernel_size,\n",
    "            style_dim,\n",
    "            upsample=upsample,\n",
    "            blur_kernel=blur_kernel,\n",
    "            demodulate=demodulate,\n",
    "        )\n",
    "\n",
    "        self.noise = NoiseInjection()\n",
    "        # self.bias = nn.Parameter(torch.zeros(1, out_channel, 1, 1))\n",
    "        # self.activate = ScaledLeakyReLU(0.2)\n",
    "        self.activate = FusedLeakyReLU(out_channel)\n",
    "\n",
    "    def forward(self, input, style, noise=None):\n",
    "        out = self.conv(input, style)\n",
    "        out = self.noise(out, noise=noise)\n",
    "        # out = out + self.bias\n",
    "        out = self.activate(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, kernel, factor=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.factor = factor\n",
    "        kernel = make_kernel(kernel) * (factor ** 2)\n",
    "        self.register_buffer('kernel', kernel)\n",
    "\n",
    "        p = kernel.shape[0] - factor\n",
    "\n",
    "        pad0 = (p + 1) // 2 + factor - 1\n",
    "        pad1 = p // 2\n",
    "\n",
    "        self.pad = (pad0, pad1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = upfirdn2d(input, self.kernel, up=self.factor, down=1, pad=self.pad)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ToRGB(nn.Module):\n",
    "    def __init__(self, in_channel, style_dim, upsample=True, blur_kernel=[1, 3, 3, 1]):\n",
    "        super().__init__()\n",
    "\n",
    "        if upsample:\n",
    "            self.upsample = Upsample(blur_kernel)\n",
    "\n",
    "        self.conv = ModulatedConv2d(in_channel, 3, 1, style_dim, demodulate=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(1, 3, 1, 1))\n",
    "\n",
    "    def forward(self, input, style, skip=None):\n",
    "        out = self.conv(input, style)\n",
    "        out = out + self.bias\n",
    "\n",
    "        if skip is not None:\n",
    "            skip = self.upsample(skip)\n",
    "\n",
    "            out = out + skip\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, blur_kernel=[1, 3, 3, 1]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = ConvLayer(in_channel, in_channel, 3)\n",
    "        self.conv2 = ConvLayer(in_channel, out_channel, 3, downsample=True)\n",
    "\n",
    "        self.skip = ConvLayer(\n",
    "            in_channel, out_channel, 1, downsample=True, activate=False, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.conv1(input)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        skip = self.skip(input)\n",
    "        out = (out + skip) / math.sqrt(2)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            size,\n",
    "            style_dim,\n",
    "            n_mlp,\n",
    "            channel_multiplier=2,\n",
    "            blur_kernel=[1, 3, 3, 1],\n",
    "            lr_mlp=0.01,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.size = size\n",
    "\n",
    "        self.style_dim = style_dim\n",
    "\n",
    "        layers = [PixelNorm()]\n",
    "\n",
    "        for i in range(n_mlp):\n",
    "            layers.append(\n",
    "                EqualLinear(\n",
    "                    style_dim, style_dim, lr_mul=lr_mlp, activation='fused_lrelu'\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.style = nn.Sequential(*layers)\n",
    "\n",
    "        self.channels = {\n",
    "            4: 512,\n",
    "            8: 512,\n",
    "            16: 512,\n",
    "            32: 512,\n",
    "            64: 256 * channel_multiplier,\n",
    "            128: 128 * channel_multiplier,\n",
    "            256: 64 * channel_multiplier,\n",
    "            512: 32 * channel_multiplier,\n",
    "            1024: 16 * channel_multiplier,\n",
    "        }\n",
    "\n",
    "        self.input = ConstantInput(self.channels[4])\n",
    "        self.conv1 = StyledConv(\n",
    "            self.channels[4], self.channels[4], 3, style_dim, blur_kernel=blur_kernel\n",
    "        )\n",
    "        self.to_rgb1 = ToRGB(self.channels[4], style_dim, upsample=False)\n",
    "\n",
    "        self.log_size = int(math.log(size, 2))\n",
    "        self.num_layers = (self.log_size - 2) * 2 + 1\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.upsamples = nn.ModuleList()\n",
    "        self.to_rgbs = nn.ModuleList()\n",
    "        self.noises = nn.Module()\n",
    "\n",
    "        in_channel = self.channels[4]\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            res = (layer_idx + 5) // 2\n",
    "            shape = [1, 1, 2 ** res, 2 ** res]\n",
    "            self.noises.register_buffer(f'noise_{layer_idx}', torch.randn(*shape))\n",
    "\n",
    "        for i in range(3, self.log_size + 1):\n",
    "            out_channel = self.channels[2 ** i]\n",
    "\n",
    "            self.convs.append(\n",
    "                StyledConv(\n",
    "                    in_channel,\n",
    "                    out_channel,\n",
    "                    3,\n",
    "                    style_dim,\n",
    "                    upsample=True,\n",
    "                    blur_kernel=blur_kernel,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.convs.append(\n",
    "                StyledConv(\n",
    "                    out_channel, out_channel, 3, style_dim, blur_kernel=blur_kernel\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.to_rgbs.append(ToRGB(out_channel, style_dim))\n",
    "\n",
    "            in_channel = out_channel\n",
    "\n",
    "        self.n_latent = self.log_size * 2 - 2\n",
    "\n",
    "    def make_noise(self):\n",
    "        device = self.input.input.device\n",
    "\n",
    "        noises = [torch.randn(1, 1, 2 ** 2, 2 ** 2, device=device)]\n",
    "\n",
    "        for i in range(3, self.log_size + 1):\n",
    "            for _ in range(2):\n",
    "                noises.append(torch.randn(1, 1, 2 ** i, 2 ** i, device=device))\n",
    "\n",
    "        return noises\n",
    "\n",
    "    def mean_latent(self, n_latent):\n",
    "        latent_in = torch.randn(\n",
    "            n_latent, self.style_dim, device=self.input.input.device\n",
    "        )\n",
    "        latent = self.style(latent_in).mean(0, keepdim=True)\n",
    "\n",
    "        return latent\n",
    "\n",
    "    def get_latent(self, input):\n",
    "        return self.style(input)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            styles,\n",
    "            return_latents=False,\n",
    "            return_features=False,\n",
    "            inject_index=None,\n",
    "            truncation=1,\n",
    "            truncation_latent=None,\n",
    "            input_is_latent=False,\n",
    "            noise=None,\n",
    "            randomize_noise=True,\n",
    "    ):\n",
    "        if not input_is_latent:\n",
    "            styles = [self.style(s) for s in styles]\n",
    "\n",
    "        if noise is None:\n",
    "            if randomize_noise:\n",
    "                noise = [None] * self.num_layers\n",
    "            else:\n",
    "                noise = [\n",
    "                    getattr(self.noises, f'noise_{i}') for i in range(self.num_layers)\n",
    "                ]\n",
    "\n",
    "        if truncation < 1:\n",
    "            style_t = []\n",
    "\n",
    "            for style in styles:\n",
    "                style_t.append(\n",
    "                    truncation_latent + truncation * (style - truncation_latent)\n",
    "                )\n",
    "\n",
    "            styles = style_t\n",
    "\n",
    "        if len(styles) < 2:\n",
    "            inject_index = self.n_latent\n",
    "\n",
    "            if styles[0].ndim < 3:\n",
    "                latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n",
    "            else:\n",
    "                latent = styles[0]\n",
    "\n",
    "        else:\n",
    "            if inject_index is None:\n",
    "                inject_index = random.randint(1, self.n_latent - 1)\n",
    "\n",
    "            latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n",
    "            latent2 = styles[1].unsqueeze(1).repeat(1, self.n_latent - inject_index, 1)\n",
    "\n",
    "            latent = torch.cat([latent, latent2], 1)\n",
    "\n",
    "        out = self.input(latent)\n",
    "        out = self.conv1(out, latent[:, 0], noise=noise[0])\n",
    "\n",
    "        skip = self.to_rgb1(out, latent[:, 1])\n",
    "\n",
    "        i = 1\n",
    "        for conv1, conv2, noise1, noise2, to_rgb in zip(\n",
    "                self.convs[::2], self.convs[1::2], noise[1::2], noise[2::2], self.to_rgbs\n",
    "        ):\n",
    "            out = conv1(out, latent[:, i], noise=noise1)\n",
    "            out = conv2(out, latent[:, i + 1], noise=noise2)\n",
    "            skip = to_rgb(out, latent[:, i + 2], skip)\n",
    "\n",
    "            i += 2\n",
    "\n",
    "        image = skip\n",
    "\n",
    "        if return_latents:\n",
    "            return image, latent\n",
    "        elif return_features:\n",
    "            return image, out\n",
    "        else:\n",
    "            return image, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define discriminator \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, size, channel_multiplier=2, blur_kernel=[1, 3, 3, 1]):\n",
    "        super().__init__()\n",
    "\n",
    "        channels = {\n",
    "            4: 512,\n",
    "            8: 512,\n",
    "            16: 512,\n",
    "            32: 512,\n",
    "            64: 256 * channel_multiplier,\n",
    "            128: 128 * channel_multiplier,\n",
    "            256: 64 * channel_multiplier,\n",
    "            512: 32 * channel_multiplier,\n",
    "            1024: 16 * channel_multiplier,\n",
    "        }\n",
    "\n",
    "        convs = [ConvLayer(3, channels[size], 1)]\n",
    "\n",
    "        log_size = int(math.log(size, 2))\n",
    "\n",
    "        in_channel = channels[size]\n",
    "\n",
    "        for i in range(log_size, 2, -1):\n",
    "            out_channel = channels[2 ** (i - 1)]\n",
    "\n",
    "            convs.append(ResBlock(in_channel, out_channel, blur_kernel))\n",
    "\n",
    "            in_channel = out_channel\n",
    "\n",
    "        self.convs = nn.Sequential(*convs)\n",
    "\n",
    "        self.stddev_group = 4\n",
    "        self.stddev_feat = 1\n",
    "\n",
    "        self.final_conv = ConvLayer(in_channel + 1, channels[4], 3)\n",
    "        self.final_linear = nn.Sequential(\n",
    "            EqualLinear(channels[4] * 4 * 4, channels[4], activation=\"fused_lrelu\"),\n",
    "            EqualLinear(channels[4], 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.convs(input)\n",
    "\n",
    "        batch, channel, height, width = out.shape\n",
    "        group = min(batch, self.stddev_group)\n",
    "        stddev = out.view(\n",
    "            group, -1, self.stddev_feat, channel // self.stddev_feat, height, width\n",
    "        )\n",
    "        stddev = torch.sqrt(stddev.var(0, unbiased=False) + 1e-8)\n",
    "        stddev = stddev.mean([2, 3, 4], keepdims=True).squeeze(2)\n",
    "        stddev = stddev.repeat(group, 1, height, width)\n",
    "        out = torch.cat([out, stddev], 1)\n",
    "\n",
    "        out = self.final_conv(out)\n",
    "\n",
    "        out = out.view(batch, -1)\n",
    "        out = self.final_linear(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "class WBLogger:\n",
    "\n",
    "    def __init__(self, args):\n",
    "        wandb_run_name = args.exp_name\n",
    "        wandb.init(project=\"style_space_run\", config=vars(args.wandb_config), name=wandb_run_name, entity=\"stylespace\")\n",
    "\n",
    "    @staticmethod\n",
    "    def log_best_model():\n",
    "        wandb.run.summary[\"best-model-save-time\"] = datetime.datetime.now()\n",
    "\n",
    "    @staticmethod\n",
    "    def log_dataset_wandb(dataset, dataset_name, device, n_images=16):\n",
    "        idxs = np.random.choice(a=range(len(dataset)), size=n_images, replace=False)\n",
    "        if device == \"cpu\":\n",
    "            data = [wandb.Image(dataset[idx][\"inputs\"].numpy().transpose(1, 2, 0)) for idx in idxs]\n",
    "        else:\n",
    "            data = [wandb.Image(dataset[idx][\"inputs\"].detach().cpu().numpy().transpose(1, 2, 0)) for idx in idxs]\n",
    "            \n",
    "        wandb.log({f\"{dataset_name} Data Samples\": data})\n",
    "\n",
    "    @staticmethod\n",
    "    # method should log all relevant metrics for a run\n",
    "    # TODO: update after code is written\n",
    "    def log(prefix, metrics_dict, global_step):\n",
    "        log_dict = {f'{prefix}_{key}': value for key, value in metrics_dict.items()} # figure out what metrics dict is\n",
    "        log_dict[\"global_step\"] = global_step\n",
    "        wandb.log(log_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ops for stylegan2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "from torch.utils.cpp_extension import load\n",
    "\n",
    "module_path = os.path.dirname(__file__) # dirname will get the parent directory name of the current file (__file__)\n",
    "# TODO: need these fused_bias files\n",
    "fused = load(\n",
    "    'fused',\n",
    "    sources=[\n",
    "        os.path.join(module_path, 'fused_bias_act.cpp'),\n",
    "        os.path.join(module_path, 'fused_bias_act_kernel.cu'),\n",
    "    ],\n",
    ")\n",
    "\n",
    "class FusedLeakyReLUFunctionBackward(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, grad_output, out, negative_slope, scale):\n",
    "        ctx.save_for_backward(out)\n",
    "        ctx.negative_slope = negative_slope\n",
    "        ctx.scale = scale\n",
    "\n",
    "        empty = grad_output.new_empty(0)\n",
    "\n",
    "        grad_input = fused.fused_bias_act(\n",
    "            grad_output, empty, out, 3, 1, negative_slope, scale\n",
    "        )\n",
    "\n",
    "        dim = [0]\n",
    "\n",
    "        if grad_input.ndim > 2:\n",
    "            dim += list(range(2, grad_input.ndim))\n",
    "\n",
    "        grad_bias = grad_input.sum(dim).detach()\n",
    "\n",
    "        return grad_input, grad_bias\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, gradgrad_input, gradgrad_bias):\n",
    "        out, = ctx.saved_tensors\n",
    "        gradgrad_out = fused.fused_bias_act(\n",
    "            gradgrad_input, gradgrad_bias, out, 3, 1, ctx.negative_slope, ctx.scale\n",
    "        )\n",
    "\n",
    "        return gradgrad_out, None, None, None\n",
    "\n",
    "\n",
    "class FusedLeakyReLUFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, bias, negative_slope, scale):\n",
    "        empty = input.new_empty(0)\n",
    "        out = fused.fused_bias_act(input, bias, empty, 3, 0, negative_slope, scale)\n",
    "        ctx.save_for_backward(out)\n",
    "        ctx.negative_slope = negative_slope\n",
    "        ctx.scale = scale\n",
    "\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        out, = ctx.saved_tensors\n",
    "\n",
    "        grad_input, grad_bias = FusedLeakyReLUFunctionBackward.apply(\n",
    "            grad_output, out, ctx.negative_slope, ctx.scale\n",
    "        )\n",
    "\n",
    "        return grad_input, grad_bias, None, None\n",
    "\n",
    "class FusedLeakyReLU(nn.Module):\n",
    "    def __init__(self, channel, negative_slope=0.2, scale=2 ** 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(channel))\n",
    "        self.negative_slope = negative_slope\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, input):\n",
    "        return fused_leaky_relu(input, self.bias, self.negative_slope, self.scale)\n",
    "\n",
    "def fused_leaky_relu(input, bias, negative_slope=0.2, scale=2 ** 0.5):\n",
    "    return FusedLeakyReLUFunction.apply(input, bias, negative_slope, scale)\n",
    "\n",
    "#######################################\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.utils.cpp_extension import load\n",
    "\n",
    "module_path = os.path.dirname(__file__)\n",
    "upfirdn2d_op = load(\n",
    "    'upfirdn2d',\n",
    "    sources=[\n",
    "        os.path.join(module_path, 'upfirdn2d.cpp'),\n",
    "        os.path.join(module_path, 'upfirdn2d_kernel.cu'),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "class UpFirDn2dBackward(Function):\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "            ctx, grad_output, kernel, grad_kernel, up, down, pad, g_pad, in_size, out_size\n",
    "    ):\n",
    "        up_x, up_y = up\n",
    "        down_x, down_y = down\n",
    "        g_pad_x0, g_pad_x1, g_pad_y0, g_pad_y1 = g_pad\n",
    "\n",
    "        grad_output = grad_output.reshape(-1, out_size[0], out_size[1], 1)\n",
    "\n",
    "        grad_input = upfirdn2d_op.upfirdn2d(\n",
    "            grad_output,\n",
    "            grad_kernel,\n",
    "            down_x,\n",
    "            down_y,\n",
    "            up_x,\n",
    "            up_y,\n",
    "            g_pad_x0,\n",
    "            g_pad_x1,\n",
    "            g_pad_y0,\n",
    "            g_pad_y1,\n",
    "        )\n",
    "        grad_input = grad_input.view(in_size[0], in_size[1], in_size[2], in_size[3])\n",
    "\n",
    "        ctx.save_for_backward(kernel)\n",
    "\n",
    "        pad_x0, pad_x1, pad_y0, pad_y1 = pad\n",
    "\n",
    "        ctx.up_x = up_x\n",
    "        ctx.up_y = up_y\n",
    "        ctx.down_x = down_x\n",
    "        ctx.down_y = down_y\n",
    "        ctx.pad_x0 = pad_x0\n",
    "        ctx.pad_x1 = pad_x1\n",
    "        ctx.pad_y0 = pad_y0\n",
    "        ctx.pad_y1 = pad_y1\n",
    "        ctx.in_size = in_size\n",
    "        ctx.out_size = out_size\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, gradgrad_input):\n",
    "        kernel, = ctx.saved_tensors\n",
    "\n",
    "        gradgrad_input = gradgrad_input.reshape(-1, ctx.in_size[2], ctx.in_size[3], 1)\n",
    "\n",
    "        gradgrad_out = upfirdn2d_op.upfirdn2d(\n",
    "            gradgrad_input,\n",
    "            kernel,\n",
    "            ctx.up_x,\n",
    "            ctx.up_y,\n",
    "            ctx.down_x,\n",
    "            ctx.down_y,\n",
    "            ctx.pad_x0,\n",
    "            ctx.pad_x1,\n",
    "            ctx.pad_y0,\n",
    "            ctx.pad_y1,\n",
    "        )\n",
    "        # gradgrad_out = gradgrad_out.view(ctx.in_size[0], ctx.out_size[0], ctx.out_size[1], ctx.in_size[3])\n",
    "        gradgrad_out = gradgrad_out.view(\n",
    "            ctx.in_size[0], ctx.in_size[1], ctx.out_size[0], ctx.out_size[1]\n",
    "        )\n",
    "\n",
    "        return gradgrad_out, None, None, None, None, None, None, None, None\n",
    "\n",
    "\n",
    "class UpFirDn2d(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, kernel, up, down, pad):\n",
    "        up_x, up_y = up\n",
    "        down_x, down_y = down\n",
    "        pad_x0, pad_x1, pad_y0, pad_y1 = pad\n",
    "\n",
    "        kernel_h, kernel_w = kernel.shape\n",
    "        batch, channel, in_h, in_w = input.shape\n",
    "        ctx.in_size = input.shape\n",
    "\n",
    "        input = input.reshape(-1, in_h, in_w, 1)\n",
    "\n",
    "        ctx.save_for_backward(kernel, torch.flip(kernel, [0, 1]))\n",
    "\n",
    "        out_h = (in_h * up_y + pad_y0 + pad_y1 - kernel_h) // down_y + 1\n",
    "        out_w = (in_w * up_x + pad_x0 + pad_x1 - kernel_w) // down_x + 1\n",
    "        ctx.out_size = (out_h, out_w)\n",
    "\n",
    "        ctx.up = (up_x, up_y)\n",
    "        ctx.down = (down_x, down_y)\n",
    "        ctx.pad = (pad_x0, pad_x1, pad_y0, pad_y1)\n",
    "\n",
    "        g_pad_x0 = kernel_w - pad_x0 - 1\n",
    "        g_pad_y0 = kernel_h - pad_y0 - 1\n",
    "        g_pad_x1 = in_w * up_x - out_w * down_x + pad_x0 - up_x + 1\n",
    "        g_pad_y1 = in_h * up_y - out_h * down_y + pad_y0 - up_y + 1\n",
    "\n",
    "        ctx.g_pad = (g_pad_x0, g_pad_x1, g_pad_y0, g_pad_y1)\n",
    "\n",
    "        out = upfirdn2d_op.upfirdn2d(\n",
    "            input, kernel, up_x, up_y, down_x, down_y, pad_x0, pad_x1, pad_y0, pad_y1\n",
    "        )\n",
    "        # out = out.view(major, out_h, out_w, minor)\n",
    "        out = out.view(-1, channel, out_h, out_w)\n",
    "\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        kernel, grad_kernel = ctx.saved_tensors\n",
    "\n",
    "        grad_input = UpFirDn2dBackward.apply(\n",
    "            grad_output,\n",
    "            kernel,\n",
    "            grad_kernel,\n",
    "            ctx.up,\n",
    "            ctx.down,\n",
    "            ctx.pad,\n",
    "            ctx.g_pad,\n",
    "            ctx.in_size,\n",
    "            ctx.out_size,\n",
    "        )\n",
    "\n",
    "        return grad_input, None, None, None, None\n",
    "\n",
    "\n",
    "def upfirdn2d(input, kernel, up=1, down=1, pad=(0, 0)):\n",
    "    out = UpFirDn2d.apply(\n",
    "        input, kernel, (up, up), (down, down), (pad[0], pad[1], pad[0], pad[1])\n",
    "    )\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def upfirdn2d_native(\n",
    "        input, kernel, up_x, up_y, down_x, down_y, pad_x0, pad_x1, pad_y0, pad_y1\n",
    "):\n",
    "    _, in_h, in_w, minor = input.shape\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "\n",
    "    out = input.view(-1, in_h, 1, in_w, 1, minor)\n",
    "    out = F.pad(out, [0, 0, 0, up_x - 1, 0, 0, 0, up_y - 1])\n",
    "    out = out.view(-1, in_h * up_y, in_w * up_x, minor)\n",
    "\n",
    "    out = F.pad(\n",
    "        out, [0, 0, max(pad_x0, 0), max(pad_x1, 0), max(pad_y0, 0), max(pad_y1, 0)]\n",
    "    )\n",
    "    out = out[\n",
    "          :,\n",
    "          max(-pad_y0, 0): out.shape[1] - max(-pad_y1, 0),\n",
    "          max(-pad_x0, 0): out.shape[2] - max(-pad_x1, 0),\n",
    "          :,\n",
    "          ]\n",
    "\n",
    "    out = out.permute(0, 3, 1, 2)\n",
    "    out = out.reshape(\n",
    "        [-1, 1, in_h * up_y + pad_y0 + pad_y1, in_w * up_x + pad_x0 + pad_x1]\n",
    "    )\n",
    "    w = torch.flip(kernel, [0, 1]).view(1, 1, kernel_h, kernel_w)\n",
    "    out = F.conv2d(out, w)\n",
    "    out = out.reshape(\n",
    "        -1,\n",
    "        minor,\n",
    "        in_h * up_y + pad_y0 + pad_y1 - kernel_h + 1,\n",
    "        in_w * up_x + pad_x0 + pad_x1 - kernel_w + 1,\n",
    "    )\n",
    "    out = out.permute(0, 2, 3, 1)\n",
    "\n",
    "    return out[:, ::down_y, ::down_x, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "from torch.nn import Conv2d, BatchNorm2d, PReLU, ReLU, Sigmoid, MaxPool2d, AdaptiveAvgPool2d, Sequential, Module\n",
    "\n",
    "def get_blocks(num_layers):\n",
    "\tif num_layers == 50:\n",
    "\t\tblocks = [\n",
    "\t\t\tget_block(in_channel=64, depth=64, num_units=3),\n",
    "\t\t\tget_block(in_channel=64, depth=128, num_units=4),\n",
    "\t\t\tget_block(in_channel=128, depth=256, num_units=14),\n",
    "\t\t\tget_block(in_channel=256, depth=512, num_units=3)\n",
    "\t\t]\n",
    "\telif num_layers == 100:\n",
    "\t\tblocks = [\n",
    "\t\t\tget_block(in_channel=64, depth=64, num_units=3),\n",
    "\t\t\tget_block(in_channel=64, depth=128, num_units=13),\n",
    "\t\t\tget_block(in_channel=128, depth=256, num_units=30),\n",
    "\t\t\tget_block(in_channel=256, depth=512, num_units=3)\n",
    "\t\t]\n",
    "\telif num_layers == 152:\n",
    "\t\tblocks = [\n",
    "\t\t\tget_block(in_channel=64, depth=64, num_units=3),\n",
    "\t\t\tget_block(in_channel=64, depth=128, num_units=8),\n",
    "\t\t\tget_block(in_channel=128, depth=256, num_units=36),\n",
    "\t\t\tget_block(in_channel=256, depth=512, num_units=3)\n",
    "\t\t]\n",
    "\telse:\n",
    "\t\traise ValueError(\"Invalid number of layers: {}. Must be one of [50, 100, 152]\".format(num_layers))\n",
    "\treturn blocks\n",
    "\n",
    "class Bottleneck(namedtuple('Block', ['in_channel', 'depth', 'stride'])):\n",
    "\t\"\"\" A named tuple describing a ResNet block. \"\"\"\n",
    "\n",
    "def get_block(in_channel, depth, num_units, stride=2):\n",
    "\treturn [Bottleneck(in_channel, depth, stride)] + [Bottleneck(depth, depth, 1) for i in range(num_units - 1)]\n",
    "\n",
    "class bottleneck_IR(Module):\n",
    "\tdef __init__(self, in_channel, depth, stride):\n",
    "\t\tsuper(bottleneck_IR, self).__init__()\n",
    "\t\tif in_channel == depth:\n",
    "\t\t\tself.shortcut_layer = MaxPool2d(1, stride)\n",
    "\t\telse:\n",
    "\t\t\tself.shortcut_layer = Sequential(\n",
    "\t\t\t\tConv2d(in_channel, depth, (1, 1), stride, bias=False),\n",
    "\t\t\t\tBatchNorm2d(depth)\n",
    "\t\t\t)\n",
    "\t\tself.res_layer = Sequential(\n",
    "\t\t\tBatchNorm2d(in_channel),\n",
    "\t\t\tConv2d(in_channel, depth, (3, 3), (1, 1), 1, bias=False), PReLU(depth),\n",
    "\t\t\tConv2d(depth, depth, (3, 3), stride, 1, bias=False), BatchNorm2d(depth)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tshortcut = self.shortcut_layer(x)\n",
    "\t\tres = self.res_layer(x)\n",
    "\t\treturn res + shortcut\n",
    "\n",
    "class SEModule(Module):\n",
    "\tdef __init__(self, channels, reduction):\n",
    "\t\tsuper(SEModule, self).__init__()\n",
    "\t\tself.avg_pool = AdaptiveAvgPool2d(1)\n",
    "\t\tself.fc1 = Conv2d(channels, channels // reduction, kernel_size=1, padding=0, bias=False)\n",
    "\t\tself.relu = ReLU(inplace=True)\n",
    "\t\tself.fc2 = Conv2d(channels // reduction, channels, kernel_size=1, padding=0, bias=False)\n",
    "\t\tself.sigmoid = Sigmoid()\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tmodule_input = x\n",
    "\t\tx = self.avg_pool(x)\n",
    "\t\tx = self.fc1(x)\n",
    "\t\tx = self.relu(x)\n",
    "\t\tx = self.fc2(x)\n",
    "\t\tx = self.sigmoid(x)\n",
    "\t\treturn module_input * x\n",
    "\n",
    "\n",
    "class bottleneck_IR_SE(Module):\n",
    "\tdef __init__(self, in_channel, depth, stride):\n",
    "\t\tsuper(bottleneck_IR_SE, self).__init__()\n",
    "\t\tif in_channel == depth:\n",
    "\t\t\tself.shortcut_layer = MaxPool2d(1, stride)\n",
    "\t\telse:\n",
    "\t\t\tself.shortcut_layer = Sequential(\n",
    "\t\t\t\tConv2d(in_channel, depth, (1, 1), stride, bias=False),\n",
    "\t\t\t\tBatchNorm2d(depth)\n",
    "\t\t\t)\n",
    "\t\tself.res_layer = Sequential(\n",
    "\t\t\tBatchNorm2d(in_channel),\n",
    "\t\t\tConv2d(in_channel, depth, (3, 3), (1, 1), 1, bias=False),\n",
    "\t\t\tPReLU(depth),\n",
    "\t\t\tConv2d(depth, depth, (3, 3), stride, 1, bias=False),\n",
    "\t\t\tBatchNorm2d(depth),\n",
    "\t\t\tSEModule(depth, 16)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tshortcut = self.shortcut_layer(x)\n",
    "\t\tres = self.res_layer(x)\n",
    "\t\treturn res + shortcut\n",
    "\n",
    "class Flatten(Module):\n",
    "\tdef forward(self, input):\n",
    "\t\treturn input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "def l2_norm(input, axis=1):\n",
    "\tnorm = torch.norm(input, 2, axis, True)\n",
    "\toutput = torch.div(input, norm)\n",
    "\treturn output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12c6f07df854dc6c88addbe025d1c4b6056d594a7c27e7c64a55d4c278f1201f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('styleSpace')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
