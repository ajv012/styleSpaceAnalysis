{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Anurag Vaidya  \n",
    "Date: 2/4/2022  \n",
    "Lab: Polina Lab @ CSAIL  \n",
    "Purpose: Create a basic classifier for the afhq dataset\n",
    "\n",
    "## Notebook Structure\n",
    "- Imports\n",
    "- Args\n",
    "- Dataset\n",
    "- Model\n",
    "- Training/ Val loop\n",
    "- main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as Fun\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from argparse import Namespace\n",
    "import time, copy\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import wandb\n",
    "# wandb.init(project=\"cat-dog-styleSpace\", entity=\"ajv012\")\n",
    "\n",
    "\n",
    "sys.path.append(\"./\")\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "                device_ids = [2],\n",
    "                device = \"cuda:2\", # device_ids[0]\n",
    "                train_dir = \"../../data/afhq/train\",\n",
    "                val_dir = \"../../data/afhq/val\",\n",
    "                save_path = \"../checkpoints/cat_dog_parallel\",\n",
    "                seed = 7,\n",
    "                labels = [\"cat\", \"dog\"],\n",
    "                batch_size = 64,\n",
    "                epochs = 50,\n",
    "                num_workers = 0,\n",
    "                class_names = {0:\"cat\", 1:\"dog\"} ,\n",
    "                lr = 0.0001,\n",
    "                momentum = 0.9,\n",
    "                criterion = nn.CrossEntropyLoss(),\n",
    "                optimizer = \"SGD\",\n",
    "                scheduler = \"STEP\",\n",
    "                scheduler_step_size = 7,\n",
    "                scheduler_gamma = 0.1,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch import distributed as dist\n",
    "\n",
    "\n",
    "def get_rank():\n",
    "    if not dist.is_available():\n",
    "        return 0\n",
    "\n",
    "    if not dist.is_initialized():\n",
    "        return 0\n",
    "\n",
    "    return dist.get_rank()\n",
    "\n",
    "\n",
    "def synchronize():\n",
    "    if not dist.is_available():\n",
    "        return\n",
    "\n",
    "    if not dist.is_initialized():\n",
    "        return\n",
    "\n",
    "    world_size = dist.get_world_size()\n",
    "\n",
    "    if world_size == 1:\n",
    "        return\n",
    "\n",
    "    dist.barrier()\n",
    "\n",
    "\n",
    "def get_world_size():\n",
    "    if not dist.is_available():\n",
    "        return 1\n",
    "\n",
    "    if not dist.is_initialized():\n",
    "        return 1\n",
    "\n",
    "    return dist.get_world_size()\n",
    "\n",
    "\n",
    "def reduce_sum(tensor):\n",
    "    if not dist.is_available():\n",
    "        return tensor\n",
    "\n",
    "    if not dist.is_initialized():\n",
    "        return tensor\n",
    "\n",
    "    tensor = tensor.clone()\n",
    "    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def gather_grad(params):\n",
    "    world_size = get_world_size()\n",
    "    \n",
    "    if world_size == 1:\n",
    "        return\n",
    "\n",
    "    for param in params:\n",
    "        if param.grad is not None:\n",
    "            dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)\n",
    "            param.grad.data.div_(world_size)\n",
    "\n",
    "\n",
    "def all_gather(data):\n",
    "    world_size = get_world_size()\n",
    "\n",
    "    if world_size == 1:\n",
    "        return [data]\n",
    "\n",
    "    buffer = pickle.dumps(data)\n",
    "    storage = torch.ByteStorage.from_buffer(buffer)\n",
    "    tensor = torch.ByteTensor(storage).to('cuda')\n",
    "\n",
    "    local_size = torch.IntTensor([tensor.numel()]).to('cuda')\n",
    "    size_list = [torch.IntTensor([0]).to('cuda') for _ in range(world_size)]\n",
    "    dist.all_gather(size_list, local_size)\n",
    "    size_list = [int(size.item()) for size in size_list]\n",
    "    max_size = max(size_list)\n",
    "\n",
    "    tensor_list = []\n",
    "    for _ in size_list:\n",
    "        tensor_list.append(torch.ByteTensor(size=(max_size,)).to('cuda'))\n",
    "\n",
    "    if local_size != max_size:\n",
    "        padding = torch.ByteTensor(size=(max_size - local_size,)).to('cuda')\n",
    "        tensor = torch.cat((tensor, padding), 0)\n",
    "\n",
    "    dist.all_gather(tensor_list, tensor)\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    for size, tensor in zip(size_list, tensor_list):\n",
    "        buffer = tensor.cpu().numpy().tobytes()[:size]\n",
    "        data_list.append(pickle.loads(buffer))\n",
    "\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def reduce_loss_dict(loss_dict):\n",
    "    world_size = get_world_size()\n",
    "\n",
    "    if world_size < 2:\n",
    "        return loss_dict\n",
    "\n",
    "    with torch.no_grad():\n",
    "        keys = []\n",
    "        losses = []\n",
    "\n",
    "        for k in sorted(loss_dict.keys()):\n",
    "            keys.append(k)\n",
    "            losses.append(loss_dict[k])\n",
    "\n",
    "        losses = torch.stack(losses, 0)\n",
    "        dist.reduce(losses, dst=0)\n",
    "\n",
    "        if dist.get_rank() == 0:\n",
    "            losses /= world_size\n",
    "\n",
    "        reduced_losses = {k: v for k, v in zip(keys, losses)}\n",
    "\n",
    "    return reduced_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class afhq_dataset(Dataset):\n",
    "    r\"\"\"\n",
    "    Take a root dir and return the transformed img and associated label with it\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, seed, labels, img_transform=None):\n",
    "\n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        # this dir has two sub dirs cat and dog. Need to combine them\n",
    "        self.root_dir = root_dir\n",
    "        self.cat_names = os.listdir(os.path.join(self.root_dir, \"cat\"))\n",
    "        self.dog_names = os.listdir(os.path.join(self.root_dir, \"dog\"))\n",
    "        self.all_names = np.asarray(self.cat_names + self.dog_names)\n",
    "        np.random.shuffle(self.all_names)\n",
    "        self.img_transform = img_transform\n",
    "        self.labels = {}\n",
    "        for i in range(len(labels)):\n",
    "            self.labels[labels[i]] = i\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        curr_path = os.path.join(self.root_dir, self.all_names[idx].strip().split(\"_\")[1], self.all_names[idx])\n",
    "        curr_img = Image.open(curr_path)\n",
    "        curr_label = self.labels[self.all_names[idx].strip().split(\"_\")[1]]\n",
    "        \n",
    "        if self.img_transform:\n",
    "            curr_img_transformed = self.img_transform(curr_img)\n",
    "        \n",
    "        return {\"inputs\" : curr_img_transformed, \"labels\" : curr_label} \n",
    "    \n",
    "    def viz_img(self, imgs):\n",
    "        r\"\"\"\n",
    "        Take a tensor or list of tensors and visualize it\n",
    "        \"\"\"\n",
    "        if not isinstance(imgs, list):\n",
    "            imgs = [imgs]\n",
    "        fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "        for i, img in enumerate(imgs):\n",
    "            img = img.detach()\n",
    "            img = F.to_pil_image(img)\n",
    "            axs[0, i].imshow(np.asarray(img))\n",
    "            axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    \n",
    "    def what_labels_mean(self):\n",
    "        return [label + \": \" + str(self.labels[label]) for label in self.labels]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class clf(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    A simple encoder and fully connected layer for classification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(clf, self).__init__()\n",
    "        self.model_ft = models.resnet18(pretrained=True)\n",
    "        self.num_ftrs = self.model_ft.fc.in_features\n",
    "        self.model_ft.fc = nn.Linear(self.num_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model_ft(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_val_model(model, datasets, dataloaders, device, criterion, optimizer, scheduler, PATH, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        running_loss_train = 0.0\n",
    "        running_corrects_train = 0\n",
    "\n",
    "        running_loss_val = 0.0\n",
    "        running_corrects_val = 0\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            since = time.time()\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            # Iterate over data.\n",
    "            for batch in dataloaders[phase]:\n",
    "                inputs, labels = batch[\"inputs\"], batch[\"labels\"]\n",
    "                inputs = inputs.to(f'cuda:{model.device_ids[0]}')\n",
    "                labels = labels.to(f'cuda:{model.device_ids[0]}')\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                if phase == \"train\":\n",
    "                    running_loss_train += loss.item() * inputs.size(0)\n",
    "                    running_corrects_train += torch.sum(preds == labels.data)\n",
    "                else:\n",
    "                    running_loss_val += loss.item() * inputs.size(0)\n",
    "                    running_corrects_val += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            if phase == \"train\":\n",
    "                epoch_loss = running_loss_train / len(datasets[phase])\n",
    "                epoch_acc = running_corrects_train.double() / len(datasets[phase])\n",
    "                # wandb.log({\"train_epoch_loss\": epoch_loss, \"train_epoch_acc\": epoch_acc})\n",
    "            else:\n",
    "                epoch_loss = running_loss_val / len(datasets[phase])\n",
    "                epoch_acc = running_corrects_val.double() / len(datasets[phase])\n",
    "                # wandb.log({\"val_epoch_loss\": epoch_loss, \"val_epoch_acc\": epoch_acc})\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            print(\"Time for epoch: {}\".format(time.time() - since))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                # save current best model\n",
    "                PATH = \"{}/checkpoint_{}.pt\".format(args.save_path, epoch)\n",
    "                torch.save({\n",
    "                            'epoch': epoch,\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'loss': epoch_loss,\n",
    "                            'acc' : epoch_acc,\n",
    "                            }, PATH)\n",
    "                # wandb.log({\"best_acc\": best_acc})\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, dataloaders, device, class_names, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in enumerate(dataloaders['val']):\n",
    "            inputs, labels = batch[\"inputs\"], batch[\"labels\"]\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                plt.imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_transforms():\n",
    "    train_transforms = transforms.Compose([\n",
    "    transforms.Resize(512),\n",
    "    transforms.CenterCrop(512),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize(512),\n",
    "        transforms.CenterCrop(512),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    return train_transforms, val_transforms\n",
    "\n",
    "def def_datasets(args, train_transforms, val_transforms):\n",
    "    dataset_train = afhq_dataset(args.train_dir, args.seed, args.labels, train_transforms)\n",
    "    dataset_val = afhq_dataset(args.val_dir, args.seed, args.labels, val_transforms)\n",
    "    datasets = {\"train\": dataset_train, \"val\": dataset_val}\n",
    "    dataset_sizes = {x: datasets[x] for x in ['train', 'val']}\n",
    "\n",
    "    return datasets, dataset_sizes\n",
    "\n",
    "def def_dataloaders(args, dataset_train, dataset_val):\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size = args.batch_size, num_workers=args.num_workers)\n",
    "    dataloader_val = DataLoader(dataset_val, batch_size = args.batch_size, num_workers=args.num_workers)\n",
    "    dataloaders = {\"train\": dataloader_train, \"val\":dataloader_val}\n",
    "\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # define transforms\n",
    "    train_transforms, val_transforms = def_transforms()\n",
    "\n",
    "    # define datasets and sizes\n",
    "    datasets, dataset_sizes = def_datasets(args, train_transforms, val_transforms)\n",
    "    \n",
    "    # define dataloaders\n",
    "    dataloaders = def_dataloaders(args, datasets[\"train\"], datasets[\"val\"])\n",
    "    \n",
    "    # define model\n",
    "    model = clf(len(args.labels))\n",
    "    model = torch.nn.DataParallel(model, device_ids=args.device_ids)\n",
    "    model.to(f'cuda:{model.device_ids[0]}')\n",
    "    \n",
    "    \n",
    "    # define criterion\n",
    "    criterion = args.criterion\n",
    "    \n",
    "    # define optim\n",
    "    if args.optimizer == \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    \n",
    "    # define lr scheduler\n",
    "    if args.scheduler == \"STEP\":\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=args.scheduler_step_size, gamma=args.scheduler_gamma)\n",
    "\n",
    "    # logging\n",
    "    # wandb.config = {\n",
    "    #                 \"learning_rate\": args.lr,\n",
    "    #                 \"epochs\": args.epochs,\n",
    "    #                 \"batch_size\": args.batch_size\n",
    "    # }\n",
    "    \n",
    "    # train and val model\n",
    "    model_final = train_and_val_model(model, datasets, dataloaders, args.device, \n",
    "                                     criterion, optimizer, scheduler, args.save_path, args.epochs)\n",
    "\n",
    "    visualize_model(model_final, dataloaders, args.device, args.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 0.4443 Acc: 0.8650\n",
      "Time for epoch: 170.36837100982666\n",
      "val Loss: 0.2045 Acc: 0.9920\n",
      "Time for epoch: 12.194339752197266\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 0.1495 Acc: 0.9940\n",
      "Time for epoch: 175.41802048683167\n",
      "val Loss: 0.0951 Acc: 0.9970\n",
      "Time for epoch: 12.10424256324768\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.0841 Acc: 0.9962\n",
      "Time for epoch: 172.47276902198792\n",
      "val Loss: 0.0579 Acc: 0.9990\n",
      "Time for epoch: 12.941193580627441\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.0571 Acc: 0.9969\n",
      "Time for epoch: 172.31546068191528\n",
      "val Loss: 0.0403 Acc: 1.0000\n",
      "Time for epoch: 11.758950471878052\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26197/451043146.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_26197/3301746511.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# train and val model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     model_final = train_and_val_model(model, datasets, dataloaders, args.device, \n\u001b[0m\u001b[1;32m     38\u001b[0m                                      criterion, optimizer, scheduler, args.save_path, args.epochs)\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_26197/1815618439.py\u001b[0m in \u001b[0;36mtrain_and_val_model\u001b[0;34m(model, datasets, dataloaders, device, criterion, optimizer, scheduler, PATH, num_epochs)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                     \u001b[0mrunning_loss_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                     \u001b[0mrunning_corrects_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9bbb49dca6fd3fb5150bec4959f4b143d84da4fc3f17c9f888243ead3e503560"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('styleSpace')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
