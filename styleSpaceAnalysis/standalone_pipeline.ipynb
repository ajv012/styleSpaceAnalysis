{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Anurag Vaidya  \n",
    "Date: 2/4/2022  \n",
    "Lab: Polina Lab @ CSAIL  \n",
    "Purpose: Create a basic classifier for the afhq dataset\n",
    "\n",
    "## Notebook Structure\n",
    "- Imports\n",
    "- Args\n",
    "- Dataset\n",
    "- Model\n",
    "- Training/ Val loop\n",
    "- main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as Fun\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from argparse import Namespace\n",
    "import time, copy\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import wandb\n",
    "# wandb.init(project=\"cat-dog-styleSpace\", entity=\"ajv012\")\n",
    "\n",
    "\n",
    "sys.path.append(\"./\")\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(device = \"cpu\", #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                 train_dir = \"../afhq/afhq/train\",\n",
    "                 val_dir = \"../afhq/afhq/val\",\n",
    "                 save_path = \"./checkpoints\",\n",
    "                 seed = 7,\n",
    "                 labels = [\"cat\", \"dog\"],\n",
    "                 batch_size = 256,\n",
    "                 epochs = 2,\n",
    "                 num_workers = 0,\n",
    "                 class_names = {0:\"cat\", 1:\"dog\"} ,\n",
    "                 lr = 0.0001,\n",
    "                 momentum = 0.9,\n",
    "                 criterion = nn.CrossEntropyLoss(),\n",
    "                 optimizer = \"SGD\",\n",
    "                 scheduler = \"STEP\",\n",
    "                 scheduler_step_size = 7,\n",
    "                 scheduler_gamma = 0.1,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class afhq_dataset(Dataset):\n",
    "    r\"\"\"\n",
    "    Take a root dir and return the transformed img and associated label with it\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, seed, labels, img_transform=None):\n",
    "\n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        # this dir has two sub dirs cat and dog. Need to combine them\n",
    "        self.root_dir = root_dir\n",
    "        self.cat_names = os.listdir(os.path.join(self.root_dir, \"cat\"))\n",
    "        self.dog_names = os.listdir(os.path.join(self.root_dir, \"dog\"))\n",
    "        self.all_names = np.asarray(self.cat_names + self.dog_names)\n",
    "        np.random.shuffle(self.all_names)\n",
    "        self.img_transform = img_transform\n",
    "        self.labels = {}\n",
    "        for i in range(len(labels)):\n",
    "            self.labels[labels[i]] = i\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        curr_path = os.path.join(self.root_dir, self.all_names[idx].strip().split(\"_\")[1], self.all_names[idx])\n",
    "        curr_img = Image.open(curr_path)\n",
    "        curr_label = self.labels[self.all_names[idx].strip().split(\"_\")[1]]\n",
    "        \n",
    "        if self.img_transform:\n",
    "            curr_img_transformed = self.img_transform(curr_img)\n",
    "        \n",
    "        return {\"inputs\" : curr_img_transformed, \"labels\" : curr_label} \n",
    "    \n",
    "    def viz_img(self, imgs):\n",
    "        r\"\"\"\n",
    "        Take a tensor or list of tensors and visualize it\n",
    "        \"\"\"\n",
    "        if not isinstance(imgs, list):\n",
    "            imgs = [imgs]\n",
    "        fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "        for i, img in enumerate(imgs):\n",
    "            img = img.detach()\n",
    "            img = F.to_pil_image(img)\n",
    "            axs[0, i].imshow(np.asarray(img))\n",
    "            axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    \n",
    "    def what_labels_mean(self):\n",
    "        return [label + \": \" + str(self.labels[label]) for label in self.labels]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class clf(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    A simple encoder and fully connected layer for classification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(clf, self).__init__()\n",
    "        self.model_ft = models.resnet18(pretrained=True)\n",
    "        self.num_ftrs = self.model_ft.fc.in_features\n",
    "        self.model_ft.fc = nn.Linear(self.num_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model_ft(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_val_model(model, datasets, dataloaders, device, criterion, optimizer, scheduler, PATH, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for batch in dataloaders[phase]:\n",
    "                inputs, labels = batch[\"inputs\"], batch[\"labels\"]\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / len(datasets[phase])\n",
    "            epoch_acc = running_corrects.double() / len(datasets[phase])\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                # save current best model\n",
    "                PATH += \"_{}.pt\".format(epoch)\n",
    "                torch.save({\n",
    "                            'epoch': epoch,\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'loss': epoch_loss,\n",
    "                            'acc' : epoch_acc,\n",
    "                            }, PATH)\n",
    "\n",
    "            # wandb.log({\"epoch_loss\": epoch_loss, \"epoch_acc\": epoch_acc})\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, dataloaders, device, class_names, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                plt.imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_transforms():\n",
    "    train_transforms = transforms.Compose([\n",
    "    transforms.Resize(512),\n",
    "    transforms.CenterCrop(512),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize(512),\n",
    "        transforms.CenterCrop(512),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    return train_transforms, val_transforms\n",
    "\n",
    "def def_datasets(args, train_transforms, val_transforms):\n",
    "    dataset_train = afhq_dataset(args.train_dir, args.seed, args.labels, train_transforms)\n",
    "    dataset_val = afhq_dataset(args.val_dir, args.seed, args.labels, val_transforms)\n",
    "    datasets = {\"train\": dataset_train, \"val\": dataset_val}\n",
    "    dataset_sizes = {x: datasets[x] for x in ['train', 'val']}\n",
    "\n",
    "    return datasets, dataset_sizes\n",
    "\n",
    "def def_samplers(args, dataset_train, dataset_val):\n",
    "    sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "    batch_sampler_train = torch.utils.data.BatchSampler(sampler_train, args.batch_size, drop_last=True)\n",
    "    sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "    return batch_sampler_train, sampler_val\n",
    "\n",
    "def def_dataloaders(args, dataset_train, dataset_val, batch_sampler_train, sampler_val):\n",
    "    dataloader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train, num_workers=args.num_workers)\n",
    "    dataloader_val = DataLoader(dataset_val, batch_sampler=sampler_val, num_workers=args.num_workers)\n",
    "    dataloaders = {\"train\": dataloader_train, \"val\":dataloader_val}\n",
    "\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # define transforms\n",
    "    train_transforms, val_transforms = def_transforms()\n",
    "\n",
    "    # define datasets and sizes\n",
    "    datasets, dataset_sizes = def_datasets(args, train_transforms, val_transforms)\n",
    "\n",
    "    # define samplers\n",
    "    batch_sampler_train, sampler_val = def_samplers(args, datasets[\"train\"], datasets[\"val\"])\n",
    "\n",
    "    # define dataloaders\n",
    "    dataloaders = def_dataloaders(args, datasets[\"train\"], datasets[\"val\"], batch_sampler_train, sampler_val)\n",
    "    \n",
    "    # define model\n",
    "    model = clf(len(args.labels))\n",
    "    \n",
    "    # define criterion\n",
    "    criterion = args.criterion\n",
    "    \n",
    "    # define optim\n",
    "    if args.optimizer == \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    \n",
    "    # define lr scheduler\n",
    "    if args.scheduler == \"STEP\":\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=args.scheduler_step_size, gamma=args.scheduler_gamma)\n",
    "\n",
    "    # logging\n",
    "    # wandb.config = {\n",
    "    #                 \"learning_rate\": args.lr,\n",
    "    #                 \"epochs\": args.epochs,\n",
    "    #                 \"batch_size\": args.batch_size\n",
    "    # }\n",
    "    \n",
    "    # train and val model\n",
    "    model_final = train_and_val_model(model, datasets, dataloaders, args.device, \n",
    "                                     criterion, optimizer, scheduler, args.save_path, args.epochs)\n",
    "\n",
    "    visualize_model(model_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9bbb49dca6fd3fb5150bec4959f4b143d84da4fc3f17c9f888243ead3e503560"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('styleSpace')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
